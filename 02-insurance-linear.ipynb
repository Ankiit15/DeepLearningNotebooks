{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Insurance cost prediction using linear regression\n",
    "\n",
    "Make a submisson here: https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans/assignment/assignment-2-train-your-first-model\n",
    "\n",
    "In this assignment we're going to use information like a person's age, sex, BMI, no. of children and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance).\n",
    "\n",
    "\n",
    "We will create a model with the following steps:\n",
    "1. Download and explore the dataset\n",
    "2. Prepare the dataset for training\n",
    "3. Create a linear regression model\n",
    "4. Train the model to fit the data\n",
    "5. Make predictions using the trained model\n",
    "\n",
    "\n",
    "This assignment builds upon the concepts from the first 2 lessons. It will help to review these Jupyter notebooks:\n",
    "- PyTorch basics: https://jovian.ai/aakashns/01-pytorch-basics\n",
    "- Linear Regression: https://jovian.ai/aakashns/02-linear-regression\n",
    "- Logistic Regression: https://jovian.ai/aakashns/03-logistic-regression\n",
    "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n",
    "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n",
    "\n",
    "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "\n",
    "# Linux / Binder\n",
    "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Windows\n",
    "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# MacOS\n",
    "# !pip install numpy matplotlib pandas torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jovian\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='02-insurance-linear-regression' # will be used by jovian.commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and explore the data\n",
    "\n",
    "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv to .\\insurance.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870afae0921c4314b24ee320f09fd199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n",
    "DATA_FILENAME = \"insurance.csv\"\n",
    "download_url(DATASET_URL, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'Ankit' # at least 5 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_dataset(dataframe_raw, rand_str):\n",
    "    dataframe = dataframe_raw.copy(deep=True)\n",
    "    # drop some rows\n",
    "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
    "    # scale input\n",
    "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n",
    "    # scale target\n",
    "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n",
    "    # drop column\n",
    "    if ord(rand_str[3]) % 2 == 1:\n",
    "        dataframe = dataframe.drop(['region'], axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>female</td>\n",
       "      <td>36.0525</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>13127.436508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>64</td>\n",
       "      <td>male</td>\n",
       "      <td>41.6955</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>15205.273466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>55</td>\n",
       "      <td>male</td>\n",
       "      <td>41.4865</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>32168.031188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>44</td>\n",
       "      <td>male</td>\n",
       "      <td>24.3485</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>8883.713145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>33</td>\n",
       "      <td>female</td>\n",
       "      <td>26.7410</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>4478.054753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex      bmi  children smoker       charges\n",
       "27     55  female  36.0525         2     no  13127.436508\n",
       "752    64    male  41.6955         0     no  15205.273466\n",
       "1258   55    male  41.4865         3     no  32168.031188\n",
       "384    44    male  24.3485         2     no   8883.713145\n",
       "406    33  female  26.7410         0     no   4478.054753"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = customize_dataset(dataframe_raw, your_name)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us answer some basic questions about the dataset. \n",
    "\n",
    "\n",
    "**Q: How many rows does the dataset have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(dataframe)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How many columns doe the dataset have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_cols = len(dataframe.columns)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of the input variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'children', 'smoker']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = dataframe.columns[:5]\n",
    "list(input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Which of the input columns are non-numeric or categorial variables ?**\n",
    "\n",
    "Hint: `sex` is one of them. List the columns that are not numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'smoker']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = []\n",
    "categorical_cols.append(dataframe.columns[1])\n",
    "categorical_cols.append(dataframe.columns[4])\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of output/target variable(s)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charges']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cols = []\n",
    "output_cols.append(dataframe.columns[-1])\n",
    "output_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n",
    "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2073d9e5548>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Charges distribution\")\n",
    "\n",
    "sns.boxplot(dataframe.charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to commit your notebook to Jovian after every step, so that you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ankitgadge250/02-insurance-linear'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the dataset for training\n",
    "\n",
    "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[55.    ,  0.    , 36.0525,  2.    ,  0.    ],\n",
       "        [64.    ,  1.    , 41.6955,  0.    ,  0.    ],\n",
       "        [55.    ,  1.    , 41.4865,  3.    ,  0.    ],\n",
       "        ...,\n",
       "        [56.    ,  1.    , 28.5285,  0.    ,  0.    ],\n",
       "        [18.    ,  0.    , 33.3355,  0.    ,  0.    ],\n",
       "        [43.    ,  1.    , 28.633 ,  0.    ,  0.    ]]),\n",
       " array([[13127.4365075],\n",
       "        [15205.2734665],\n",
       "        [32168.0311885],\n",
       "        ...,\n",
       "        [11946.9968855],\n",
       "        [ 2357.9974665],\n",
       "        [ 7315.984509 ]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1271, 5]), torch.Size([1271, 1]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs_array)\n",
    "targets = torch.from_numpy(targets_array)\n",
    "inputs = inputs.to(torch.float32)\n",
    "targets = targets.to(torch.float32)\n",
    "inputs.size(),targets.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype, targets.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.15 # between 0.1 and 0.2\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create data loaders for training & validation.\n",
    "\n",
    "**Q: Pick a batch size for the data loader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a batch of data to verify everything is working fine so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[31.0000,  0.0000, 41.9045,  1.0000,  1.0000],\n",
      "        [64.0000,  1.0000, 29.0510,  0.0000,  0.0000],\n",
      "        [20.0000,  1.0000, 30.7230,  0.0000,  0.0000],\n",
      "        [35.0000,  0.0000, 39.4460,  2.0000,  0.0000],\n",
      "        [32.0000,  1.0000, 33.0330,  1.0000,  0.0000],\n",
      "        [54.0000,  1.0000, 26.4385,  0.0000,  0.0000],\n",
      "        [40.0000,  0.0000, 36.0525,  2.0000,  1.0000],\n",
      "        [43.0000,  1.0000, 28.0720,  5.0000,  0.0000],\n",
      "        [41.0000,  0.0000, 34.7600,  0.0000,  0.0000],\n",
      "        [38.0000,  1.0000, 32.1860,  2.0000,  0.0000],\n",
      "        [47.0000,  0.0000, 32.3070,  1.0000,  0.0000],\n",
      "        [55.0000,  0.0000, 33.1540,  2.0000,  0.0000],\n",
      "        [41.0000,  1.0000, 26.3340,  1.0000,  0.0000],\n",
      "        [22.0000,  0.0000, 31.7020,  0.0000,  0.0000],\n",
      "        [19.0000,  1.0000, 22.3300,  0.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 36.4705,  0.0000,  0.0000],\n",
      "        [62.0000,  0.0000, 40.5460,  1.0000,  0.0000],\n",
      "        [61.0000,  1.0000, 35.5300,  2.0000,  0.0000],\n",
      "        [54.0000,  1.0000, 27.6100,  3.0000,  1.0000],\n",
      "        [32.0000,  1.0000, 31.8230,  1.0000,  1.0000],\n",
      "        [50.0000,  1.0000, 35.4255,  0.0000,  0.0000],\n",
      "        [25.0000,  0.0000, 46.3430,  1.0000,  0.0000],\n",
      "        [36.0000,  1.0000, 36.7400,  2.0000,  1.0000],\n",
      "        [18.0000,  1.0000, 36.8885,  0.0000,  1.0000],\n",
      "        [19.0000,  0.0000, 32.7800,  0.0000,  0.0000],\n",
      "        [23.0000,  1.0000, 19.1235,  1.0000,  0.0000],\n",
      "        [20.0000,  1.0000, 32.7085,  0.0000,  0.0000],\n",
      "        [42.0000,  0.0000, 27.8300,  1.0000,  0.0000],\n",
      "        [40.0000,  0.0000, 39.8090,  0.0000,  0.0000],\n",
      "        [53.0000,  0.0000, 27.2745,  1.0000,  0.0000],\n",
      "        [19.0000,  0.0000, 30.7230,  3.0000,  0.0000],\n",
      "        [32.0000,  1.0000, 31.7680,  0.0000,  0.0000],\n",
      "        [57.0000,  0.0000, 24.4530,  0.0000,  0.0000],\n",
      "        [25.0000,  1.0000, 30.3050,  0.0000,  0.0000],\n",
      "        [37.0000,  0.0000, 38.2800,  2.0000,  1.0000],\n",
      "        [23.0000,  0.0000, 43.1970,  2.0000,  0.0000],\n",
      "        [24.0000,  0.0000, 33.1100,  3.0000,  0.0000],\n",
      "        [44.0000,  1.0000, 43.4720,  0.0000,  0.0000],\n",
      "        [26.0000,  1.0000, 35.7390,  1.0000,  0.0000],\n",
      "        [57.0000,  0.0000, 31.6635,  4.0000,  0.0000],\n",
      "        [56.0000,  0.0000, 41.2610,  2.0000,  0.0000],\n",
      "        [38.0000,  1.0000, 40.7550,  1.0000,  0.0000],\n",
      "        [28.0000,  0.0000, 28.9465,  3.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 43.0760,  0.0000,  0.0000],\n",
      "        [53.0000,  1.0000, 45.6170,  0.0000,  0.0000],\n",
      "        [36.0000,  0.0000, 21.8405,  0.0000,  0.0000],\n",
      "        [19.0000,  1.0000, 35.1120,  0.0000,  1.0000],\n",
      "        [55.0000,  0.0000, 40.8100,  0.0000,  0.0000],\n",
      "        [59.0000,  1.0000, 34.9690,  2.0000,  0.0000],\n",
      "        [54.0000,  1.0000, 33.2310,  0.0000,  0.0000],\n",
      "        [57.0000,  0.0000, 28.3140,  2.0000,  0.0000],\n",
      "        [43.0000,  0.0000, 50.8200,  0.0000,  1.0000],\n",
      "        [47.0000,  0.0000, 29.2600,  2.0000,  0.0000],\n",
      "        [26.0000,  0.0000, 24.8710,  0.0000,  0.0000],\n",
      "        [32.0000,  1.0000, 36.9930,  1.0000,  1.0000],\n",
      "        [57.0000,  0.0000, 34.2760,  0.0000,  1.0000],\n",
      "        [58.0000,  1.0000, 37.8290,  0.0000,  0.0000],\n",
      "        [53.0000,  0.0000, 41.8660,  3.0000,  0.0000],\n",
      "        [59.0000,  0.0000, 30.6130,  3.0000,  0.0000],\n",
      "        [35.0000,  1.0000, 26.5430,  1.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 39.1875,  0.0000,  0.0000],\n",
      "        [59.0000,  0.0000, 35.6345,  3.0000,  0.0000],\n",
      "        [53.0000,  0.0000, 36.5750,  0.0000,  0.0000],\n",
      "        [19.0000,  0.0000, 19.5800,  0.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 34.2430,  0.0000,  0.0000],\n",
      "        [22.0000,  0.0000, 33.4400,  0.0000,  0.0000],\n",
      "        [48.0000,  1.0000, 33.8580,  3.0000,  0.0000],\n",
      "        [43.0000,  0.0000, 33.7535,  2.0000,  0.0000],\n",
      "        [64.0000,  0.0000, 36.2615,  0.0000,  0.0000],\n",
      "        [37.0000,  0.0000, 52.3600,  2.0000,  1.0000],\n",
      "        [35.0000,  1.0000, 26.8620,  3.0000,  1.0000],\n",
      "        [37.0000,  1.0000, 33.9625,  3.0000,  0.0000],\n",
      "        [64.0000,  0.0000, 43.6700,  0.0000,  0.0000],\n",
      "        [62.0000,  1.0000, 41.1400,  0.0000,  0.0000],\n",
      "        [23.0000,  1.0000, 26.2295,  0.0000,  0.0000],\n",
      "        [54.0000,  0.0000, 23.6170,  3.0000,  0.0000],\n",
      "        [50.0000,  0.0000, 28.1600,  0.0000,  0.0000],\n",
      "        [48.0000,  0.0000, 25.0800,  0.0000,  0.0000],\n",
      "        [18.0000,  0.0000, 34.4850,  0.0000,  0.0000],\n",
      "        [22.0000,  1.0000, 34.4850,  1.0000,  0.0000],\n",
      "        [48.0000,  0.0000, 35.5300,  2.0000,  0.0000],\n",
      "        [45.0000,  1.0000, 22.3850,  3.0000,  0.0000],\n",
      "        [38.0000,  1.0000, 42.2290,  3.0000,  1.0000],\n",
      "        [64.0000,  1.0000, 37.9500,  0.0000,  0.0000],\n",
      "        [41.0000,  0.0000, 35.8600,  3.0000,  0.0000],\n",
      "        [24.0000,  1.0000, 35.2110,  0.0000,  0.0000],\n",
      "        [31.0000,  0.0000, 23.9305,  0.0000,  0.0000],\n",
      "        [30.0000,  0.0000, 26.0205,  3.0000,  1.0000],\n",
      "        [23.0000,  1.0000, 35.8160,  0.0000,  0.0000],\n",
      "        [21.0000,  1.0000, 26.1250,  2.0000,  0.0000],\n",
      "        [28.0000,  0.0000, 30.2500,  2.0000,  0.0000],\n",
      "        [44.0000,  1.0000, 41.8660,  1.0000,  0.0000],\n",
      "        [51.0000,  0.0000, 41.8660,  0.0000,  1.0000],\n",
      "        [62.0000,  1.0000, 43.9230,  0.0000,  0.0000],\n",
      "        [51.0000,  1.0000, 47.1900,  2.0000,  1.0000],\n",
      "        [19.0000,  1.0000, 28.1105,  1.0000,  0.0000],\n",
      "        [35.0000,  1.0000, 33.5500,  1.0000,  0.0000],\n",
      "        [56.0000,  1.0000, 44.3300,  0.0000,  0.0000],\n",
      "        [37.0000,  0.0000, 32.4500,  2.0000,  0.0000],\n",
      "        [19.0000,  0.0000, 27.1700,  0.0000,  0.0000],\n",
      "        [20.0000,  1.0000, 33.1265,  5.0000,  0.0000],\n",
      "        [45.0000,  0.0000, 33.5445,  1.0000,  1.0000],\n",
      "        [25.0000,  0.0000, 35.4530,  1.0000,  0.0000],\n",
      "        [30.0000,  1.0000, 30.4095,  1.0000,  0.0000],\n",
      "        [18.0000,  1.0000, 41.0190,  0.0000,  0.0000],\n",
      "        [40.0000,  1.0000, 28.9465,  1.0000,  0.0000],\n",
      "        [35.0000,  0.0000, 34.1000,  1.0000,  0.0000],\n",
      "        [63.0000,  1.0000, 43.7800,  3.0000,  0.0000],\n",
      "        [31.0000,  0.0000, 40.2930,  2.0000,  0.0000],\n",
      "        [64.0000,  0.0000, 33.1265,  3.0000,  0.0000],\n",
      "        [58.0000,  1.0000, 53.9660,  0.0000,  0.0000],\n",
      "        [24.0000,  0.0000, 25.5310,  0.0000,  0.0000],\n",
      "        [43.0000,  0.0000, 27.1700,  2.0000,  1.0000],\n",
      "        [21.0000,  0.0000, 19.1400,  1.0000,  0.0000],\n",
      "        [55.0000,  0.0000, 44.8910,  3.0000,  0.0000],\n",
      "        [22.0000,  1.0000, 37.1470,  0.0000,  0.0000],\n",
      "        [26.0000,  0.0000, 32.4280,  1.0000,  0.0000],\n",
      "        [19.0000,  1.0000, 37.5100,  0.0000,  0.0000],\n",
      "        [32.0000,  1.0000, 33.8800,  3.0000,  0.0000],\n",
      "        [38.0000,  0.0000, 21.9450,  2.0000,  0.0000],\n",
      "        [37.0000,  0.0000, 42.2290,  0.0000,  1.0000],\n",
      "        [61.0000,  0.0000, 31.9770,  0.0000,  1.0000],\n",
      "        [53.0000,  0.0000, 29.3700,  2.0000,  0.0000],\n",
      "        [54.0000,  0.0000, 52.1510,  0.0000,  1.0000],\n",
      "        [27.0000,  1.0000, 36.4705,  2.0000,  0.0000],\n",
      "        [25.0000,  1.0000, 29.3645,  4.0000,  0.0000],\n",
      "        [44.0000,  1.0000, 24.0350,  3.0000,  0.0000],\n",
      "        [47.0000,  1.0000, 31.0365,  4.0000,  0.0000],\n",
      "        [46.0000,  0.0000, 31.7900,  2.0000,  0.0000],\n",
      "        [29.0000,  0.0000, 28.4900,  0.0000,  0.0000],\n",
      "        [47.0000,  0.0000, 32.4995,  1.0000,  0.0000],\n",
      "        [22.0000,  1.0000, 21.9450,  3.0000,  0.0000],\n",
      "        [52.0000,  1.0000, 37.9335,  3.0000,  1.0000],\n",
      "        [21.0000,  0.0000, 28.3800,  0.0000,  0.0000],\n",
      "        [57.0000,  1.0000, 30.9100,  0.0000,  0.0000],\n",
      "        [30.0000,  0.0000, 47.4320,  2.0000,  0.0000],\n",
      "        [64.0000,  0.0000, 35.0075,  2.0000,  0.0000],\n",
      "        [33.0000,  0.0000, 39.9190,  3.0000,  0.0000],\n",
      "        [23.0000,  0.0000, 34.5400,  0.0000,  1.0000],\n",
      "        [58.0000,  0.0000, 27.7200,  0.0000,  0.0000],\n",
      "        [38.0000,  1.0000, 31.0970,  1.0000,  0.0000],\n",
      "        [27.0000,  0.0000, 38.2800,  1.0000,  0.0000],\n",
      "        [35.0000,  1.0000, 43.6810,  4.0000,  0.0000],\n",
      "        [41.0000,  1.0000, 31.2455,  1.0000,  0.0000],\n",
      "        [37.0000,  1.0000, 39.8090,  0.0000,  0.0000],\n",
      "        [52.0000,  1.0000, 30.0960,  0.0000,  1.0000],\n",
      "        [42.0000,  1.0000, 31.1410,  3.0000,  1.0000],\n",
      "        [36.0000,  1.0000, 34.6500,  0.0000,  0.0000],\n",
      "        [28.0000,  1.0000, 40.8100,  1.0000,  0.0000],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [64.0000,  1.0000, 43.0760,  1.0000,  0.0000]])\n",
      "targets: tensor([[62671.0508],\n",
      "        [15402.1768],\n",
      "        [ 2104.7144],\n",
      "        [ 6245.0767],\n",
      "        [ 4359.6655],\n",
      "        [11152.5205],\n",
      "        [42803.5664],\n",
      "        [15491.8135],\n",
      "        [ 6619.1558],\n",
      "        [ 6909.8926],\n",
      "        [ 9146.0293],\n",
      "        [12713.7070],\n",
      "        [ 7338.5732],\n",
      "        [ 2307.7244],\n",
      "        [ 1329.2181],\n",
      "        [ 2362.2363],\n",
      "        [33833.4023],\n",
      "        [15107.9932],\n",
      "        [27159.0586],\n",
      "        [21100.0742],\n",
      "        [ 9453.7334],\n",
      "        [ 3465.1262],\n",
      "        [41104.5586],\n",
      "        [37041.0898],\n",
      "        [ 1866.5775],\n",
      "        [ 2969.4556],\n",
      "        [ 1893.3989],\n",
      "        [ 7538.6841],\n",
      "        [ 6334.5112],\n",
      "        [11708.0811],\n",
      "        [20157.4121],\n",
      "        [ 4137.5352],\n",
      "        [12871.3369],\n",
      "        [ 2699.7913],\n",
      "        [42625.0742],\n",
      "        [ 3745.6553],\n",
      "        [ 4531.3721],\n",
      "        [ 7435.1099],\n",
      "        [ 3734.8875],\n",
      "        [15402.0059],\n",
      "        [13124.0928],\n",
      "        [ 6505.2485],\n",
      "        [ 5684.0220],\n",
      "        [ 1747.3575],\n",
      "        [10169.6123],\n",
      "        [ 5840.1099],\n",
      "        [36112.8125],\n",
      "        [11463.5986],\n",
      "        [13833.8066],\n",
      "        [10947.7051],\n",
      "        [13513.2070],\n",
      "        [49073.6289],\n",
      "        [10395.9502],\n",
      "        [ 3399.1931],\n",
      "        [40240.0547],\n",
      "        [46629.4648],\n",
      "        [12566.0098],\n",
      "        [21895.4082],\n",
      "        [14981.3770],\n",
      "        [ 5483.9810],\n",
      "        [ 2365.9099],\n",
      "        [15611.9766],\n",
      "        [11304.4268],\n",
      "        [ 1848.7300],\n",
      "        [ 1735.4144],\n",
      "        [ 2933.8843],\n",
      "        [10851.0156],\n",
      "        [ 8892.5977],\n",
      "        [15721.1562],\n",
      "        [49341.4570],\n",
      "        [20717.3379],\n",
      "        [ 7272.6436],\n",
      "        [15321.3633],\n",
      "        [13887.9131],\n",
      "        [ 2562.8335],\n",
      "        [13348.6260],\n",
      "        [ 9557.3301],\n",
      "        [ 8847.8770],\n",
      "        [ 1735.7417],\n",
      "        [ 2828.2974],\n",
      "        [10746.2764],\n",
      "        [ 9207.7363],\n",
      "        [44885.6914],\n",
      "        [14790.3994],\n",
      "        [ 8511.3330],\n",
      "        [ 2120.2927],\n",
      "        [ 4423.4683],\n",
      "        [20079.4863],\n",
      "        [ 1951.9854],\n",
      "        [ 3292.4922],\n",
      "        [21590.1074],\n",
      "        [ 7653.3584],\n",
      "        [47508.4336],\n",
      "        [13891.6758],\n",
      "        [50785.2969],\n",
      "        [ 2377.0740],\n",
      "        [ 5083.6450],\n",
      "        [11344.5518],\n",
      "        [ 6753.7886],\n",
      "        [ 1858.9923],\n",
      "        [ 5259.1143],\n",
      "        [42506.3047],\n",
      "        [19493.4336],\n",
      "        [ 4533.7256],\n",
      "        [ 1221.3463],\n",
      "        [ 6836.6343],\n",
      "        [ 5607.6187],\n",
      "        [16231.9736],\n",
      "        [ 5296.2417],\n",
      "        [17607.6074],\n",
      "        [12178.0186],\n",
      "        [26837.4922],\n",
      "        [23412.4766],\n",
      "        [ 2766.2378],\n",
      "        [13359.8066],\n",
      "        [ 1791.8566],\n",
      "        [ 3629.8308],\n",
      "        [ 1349.7429],\n",
      "        [ 5621.2705],\n",
      "        [ 7633.2759],\n",
      "        [43248.3516],\n",
      "        [31181.2559],\n",
      "        [11931.3350],\n",
      "        [68234.3594],\n",
      "        [ 4342.8223],\n",
      "        [ 5219.4399],\n",
      "        [ 9513.5195],\n",
      "        [11135.5820],\n",
      "        [ 9440.9082],\n",
      "        [ 3588.0139],\n",
      "        [ 9556.0996],\n",
      "        [ 4285.8022],\n",
      "        [64222.8984],\n",
      "        [ 2148.5012],\n",
      "        [11733.0273],\n",
      "        [ 5086.3916],\n",
      "        [17193.9199],\n",
      "        [ 7010.3726],\n",
      "        [36557.9141],\n",
      "        [12665.7607],\n",
      "        [ 5868.3799],\n",
      "        [ 3828.4590],\n",
      "        [20861.4902],\n",
      "        [ 7131.2139],\n",
      "        [20559.7344],\n",
      "        [26101.1758],\n",
      "        [35082.5820],\n",
      "        [ 4710.3892],\n",
      "        [ 3506.5623],\n",
      "        [15427.5596]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(\"inputs:\", xb)\n",
    "    print(\"targets:\", yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work by committing to Jovian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ankitgadge250/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Linear Regression Model\n",
    "\n",
    "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1271, 5]), torch.Size([1271, 1]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size(), targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)\n",
    "input_size,output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n",
    "\n",
    "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size) # fill this (hint: use input_size & output_size defined above)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear(xb)     # fill this\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.l1_loss(out, targets)       # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out, targets)     # fill this    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the weights and biases of the model using `model.parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0174,  0.0445,  0.1531, -0.2111,  0.2433]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2710], requires_grad=True)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final commit before we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ankitgadge250/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model to fit the data\n",
    "\n",
    "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 14905.6015625}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader) # Use the the evaluate function\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n",
    "\n",
    "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 8152.8965\n",
      "Epoch [40], val_loss: 8147.3584\n",
      "Epoch [60], val_loss: 8143.0479\n",
      "Epoch [80], val_loss: 8138.7314\n",
      "Epoch [100], val_loss: 8134.6504\n",
      "Epoch [120], val_loss: 8132.9092\n",
      "Epoch [140], val_loss: 8131.1260\n",
      "Epoch [160], val_loss: 8129.5508\n",
      "Epoch [180], val_loss: 8128.7148\n",
      "Epoch [200], val_loss: 8127.0146\n",
      "Epoch [220], val_loss: 8125.9023\n",
      "Epoch [240], val_loss: 8124.9243\n",
      "Epoch [260], val_loss: 8123.7944\n",
      "Epoch [280], val_loss: 8123.2783\n",
      "Epoch [300], val_loss: 8123.5547\n",
      "Epoch [320], val_loss: 8123.6611\n",
      "Epoch [340], val_loss: 8122.4219\n",
      "Epoch [360], val_loss: 8122.2026\n",
      "Epoch [380], val_loss: 8122.1226\n",
      "Epoch [400], val_loss: 8122.6016\n",
      "Epoch [420], val_loss: 8122.3970\n",
      "Epoch [440], val_loss: 8122.3398\n",
      "Epoch [460], val_loss: 8122.5938\n",
      "Epoch [480], val_loss: 8122.5459\n",
      "Epoch [500], val_loss: 8122.5498\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "lr = 1e-2\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 8124.1777\n",
      "Epoch [40], val_loss: 8122.0273\n",
      "Epoch [60], val_loss: 8120.0635\n",
      "Epoch [80], val_loss: 8117.0762\n",
      "Epoch [100], val_loss: 8114.5264\n",
      "Epoch [120], val_loss: 8115.0854\n",
      "Epoch [140], val_loss: 8114.8716\n",
      "Epoch [160], val_loss: 8110.7202\n",
      "Epoch [180], val_loss: 8109.1709\n",
      "Epoch [200], val_loss: 8106.0723\n",
      "Epoch [220], val_loss: 8106.4189\n",
      "Epoch [240], val_loss: 8103.9072\n",
      "Epoch [260], val_loss: 8099.6572\n",
      "Epoch [280], val_loss: 8099.0449\n",
      "Epoch [300], val_loss: 8098.9683\n",
      "Epoch [320], val_loss: 8100.1035\n",
      "Epoch [340], val_loss: 8092.9741\n",
      "Epoch [360], val_loss: 8094.0923\n",
      "Epoch [380], val_loss: 8090.9790\n",
      "Epoch [400], val_loss: 8088.8760\n",
      "Epoch [420], val_loss: 8086.1675\n",
      "Epoch [440], val_loss: 8085.5620\n",
      "Epoch [460], val_loss: 8085.6699\n",
      "Epoch [480], val_loss: 8082.7158\n",
      "Epoch [500], val_loss: 8080.0312\n",
      "Epoch [520], val_loss: 8082.7061\n",
      "Epoch [540], val_loss: 8077.9170\n",
      "Epoch [560], val_loss: 8075.5005\n",
      "Epoch [580], val_loss: 8075.1592\n",
      "Epoch [600], val_loss: 8075.7666\n",
      "Epoch [620], val_loss: 8077.0742\n",
      "Epoch [640], val_loss: 8074.3359\n",
      "Epoch [660], val_loss: 8070.4106\n",
      "Epoch [680], val_loss: 8066.5234\n",
      "Epoch [700], val_loss: 8066.8242\n",
      "Epoch [720], val_loss: 8063.7725\n",
      "Epoch [740], val_loss: 8063.7549\n",
      "Epoch [760], val_loss: 8061.9248\n",
      "Epoch [780], val_loss: 8061.9648\n",
      "Epoch [800], val_loss: 8060.1255\n",
      "Epoch [820], val_loss: 8061.9199\n",
      "Epoch [840], val_loss: 8059.4697\n",
      "Epoch [860], val_loss: 8057.7129\n",
      "Epoch [880], val_loss: 8059.1826\n",
      "Epoch [900], val_loss: 8054.0000\n",
      "Epoch [920], val_loss: 8055.1182\n",
      "Epoch [940], val_loss: 8052.5527\n",
      "Epoch [960], val_loss: 8053.6416\n",
      "Epoch [980], val_loss: 8049.0317\n",
      "Epoch [1000], val_loss: 8049.3545\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-1\n",
    "history2 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 8049.2319\n",
      "Epoch [40], val_loss: 8046.6392\n",
      "Epoch [60], val_loss: 8046.3408\n",
      "Epoch [80], val_loss: 8044.6064\n",
      "Epoch [100], val_loss: 8045.4800\n",
      "Epoch [120], val_loss: 8042.0112\n",
      "Epoch [140], val_loss: 8041.4341\n",
      "Epoch [160], val_loss: 8040.2920\n",
      "Epoch [180], val_loss: 8038.0977\n",
      "Epoch [200], val_loss: 8040.8906\n",
      "Epoch [220], val_loss: 8038.4746\n",
      "Epoch [240], val_loss: 8038.4575\n",
      "Epoch [260], val_loss: 8036.2354\n",
      "Epoch [280], val_loss: 8035.0332\n",
      "Epoch [300], val_loss: 8031.8154\n",
      "Epoch [320], val_loss: 8033.0762\n",
      "Epoch [340], val_loss: 8030.3975\n",
      "Epoch [360], val_loss: 8030.2871\n",
      "Epoch [380], val_loss: 8028.4268\n",
      "Epoch [400], val_loss: 8028.2646\n",
      "Epoch [420], val_loss: 8025.7896\n",
      "Epoch [440], val_loss: 8026.2178\n",
      "Epoch [460], val_loss: 8025.1094\n",
      "Epoch [480], val_loss: 8026.3052\n",
      "Epoch [500], val_loss: 8023.1094\n",
      "Epoch [520], val_loss: 8021.7969\n",
      "Epoch [540], val_loss: 8019.6680\n",
      "Epoch [560], val_loss: 8019.4229\n",
      "Epoch [580], val_loss: 8018.4678\n",
      "Epoch [600], val_loss: 8016.8853\n",
      "Epoch [620], val_loss: 8016.0957\n",
      "Epoch [640], val_loss: 8015.7090\n",
      "Epoch [660], val_loss: 8016.2617\n",
      "Epoch [680], val_loss: 8013.4590\n",
      "Epoch [700], val_loss: 8012.8701\n",
      "Epoch [720], val_loss: 8010.7666\n",
      "Epoch [740], val_loss: 8011.5073\n",
      "Epoch [760], val_loss: 8009.7163\n",
      "Epoch [780], val_loss: 8008.3516\n",
      "Epoch [800], val_loss: 8006.7808\n",
      "Epoch [820], val_loss: 8005.9077\n",
      "Epoch [840], val_loss: 8007.7422\n",
      "Epoch [860], val_loss: 8004.3770\n",
      "Epoch [880], val_loss: 8005.8140\n",
      "Epoch [900], val_loss: 8003.0928\n",
      "Epoch [920], val_loss: 8001.2490\n",
      "Epoch [940], val_loss: 8000.2002\n",
      "Epoch [960], val_loss: 7999.7275\n",
      "Epoch [980], val_loss: 7999.6172\n",
      "Epoch [1000], val_loss: 7997.1689\n",
      "Epoch [1020], val_loss: 7997.9092\n",
      "Epoch [1040], val_loss: 7995.8916\n",
      "Epoch [1060], val_loss: 7994.0449\n",
      "Epoch [1080], val_loss: 7996.9492\n",
      "Epoch [1100], val_loss: 7992.9600\n",
      "Epoch [1120], val_loss: 7992.8701\n",
      "Epoch [1140], val_loss: 7990.2007\n",
      "Epoch [1160], val_loss: 7991.6885\n",
      "Epoch [1180], val_loss: 7994.6968\n",
      "Epoch [1200], val_loss: 7988.7891\n",
      "Epoch [1220], val_loss: 7987.3379\n",
      "Epoch [1240], val_loss: 7987.0864\n",
      "Epoch [1260], val_loss: 7985.1460\n",
      "Epoch [1280], val_loss: 7985.0820\n",
      "Epoch [1300], val_loss: 7985.0449\n",
      "Epoch [1320], val_loss: 7982.1475\n",
      "Epoch [1340], val_loss: 7982.9951\n",
      "Epoch [1360], val_loss: 7979.8574\n",
      "Epoch [1380], val_loss: 7980.1162\n",
      "Epoch [1400], val_loss: 7979.3896\n",
      "Epoch [1420], val_loss: 7985.3257\n",
      "Epoch [1440], val_loss: 7980.3711\n",
      "Epoch [1460], val_loss: 7975.5513\n",
      "Epoch [1480], val_loss: 7973.7759\n",
      "Epoch [1500], val_loss: 7975.1758\n",
      "Epoch [1520], val_loss: 7975.2515\n",
      "Epoch [1540], val_loss: 7971.7939\n",
      "Epoch [1560], val_loss: 7970.4727\n",
      "Epoch [1580], val_loss: 7970.3105\n",
      "Epoch [1600], val_loss: 7972.9580\n",
      "Epoch [1620], val_loss: 7971.2178\n",
      "Epoch [1640], val_loss: 7966.6074\n",
      "Epoch [1660], val_loss: 7967.9150\n",
      "Epoch [1680], val_loss: 7966.7598\n",
      "Epoch [1700], val_loss: 7964.5835\n",
      "Epoch [1720], val_loss: 7966.0537\n",
      "Epoch [1740], val_loss: 7963.3833\n",
      "Epoch [1760], val_loss: 7966.2393\n",
      "Epoch [1780], val_loss: 7961.9497\n",
      "Epoch [1800], val_loss: 7959.6250\n",
      "Epoch [1820], val_loss: 7959.9639\n",
      "Epoch [1840], val_loss: 7964.4922\n",
      "Epoch [1860], val_loss: 7962.7690\n",
      "Epoch [1880], val_loss: 7956.2725\n",
      "Epoch [1900], val_loss: 7957.9590\n",
      "Epoch [1920], val_loss: 7954.9082\n",
      "Epoch [1940], val_loss: 7953.4697\n",
      "Epoch [1960], val_loss: 7955.8330\n",
      "Epoch [1980], val_loss: 7952.1016\n",
      "Epoch [2000], val_loss: 7952.5996\n",
      "Epoch [2020], val_loss: 7951.5107\n",
      "Epoch [2040], val_loss: 7949.3105\n",
      "Epoch [2060], val_loss: 7948.4414\n",
      "Epoch [2080], val_loss: 7952.0049\n",
      "Epoch [2100], val_loss: 7946.7749\n",
      "Epoch [2120], val_loss: 7945.9443\n",
      "Epoch [2140], val_loss: 7947.1514\n",
      "Epoch [2160], val_loss: 7945.0498\n",
      "Epoch [2180], val_loss: 7955.5137\n",
      "Epoch [2200], val_loss: 7943.3906\n",
      "Epoch [2220], val_loss: 7944.3379\n",
      "Epoch [2240], val_loss: 7941.0205\n",
      "Epoch [2260], val_loss: 7940.5591\n",
      "Epoch [2280], val_loss: 7941.5806\n",
      "Epoch [2300], val_loss: 7939.0703\n",
      "Epoch [2320], val_loss: 7938.5156\n",
      "Epoch [2340], val_loss: 7939.4805\n",
      "Epoch [2360], val_loss: 7937.7412\n",
      "Epoch [2380], val_loss: 7934.8164\n",
      "Epoch [2400], val_loss: 7934.2627\n",
      "Epoch [2420], val_loss: 7933.4541\n",
      "Epoch [2440], val_loss: 7935.1387\n",
      "Epoch [2460], val_loss: 7931.2998\n",
      "Epoch [2480], val_loss: 7941.9736\n",
      "Epoch [2500], val_loss: 7930.3213\n",
      "Epoch [2520], val_loss: 7929.8813\n",
      "Epoch [2540], val_loss: 7928.4282\n",
      "Epoch [2560], val_loss: 7928.3667\n",
      "Epoch [2580], val_loss: 7928.1128\n",
      "Epoch [2600], val_loss: 7925.6943\n",
      "Epoch [2620], val_loss: 7925.4238\n",
      "Epoch [2640], val_loss: 7927.4331\n",
      "Epoch [2660], val_loss: 7930.8408\n",
      "Epoch [2680], val_loss: 7922.8989\n",
      "Epoch [2700], val_loss: 7923.9062\n",
      "Epoch [2720], val_loss: 7924.5566\n",
      "Epoch [2740], val_loss: 7921.0029\n",
      "Epoch [2760], val_loss: 7922.3203\n",
      "Epoch [2780], val_loss: 7921.7983\n",
      "Epoch [2800], val_loss: 7918.7446\n",
      "Epoch [2820], val_loss: 7920.2363\n",
      "Epoch [2840], val_loss: 7918.6968\n",
      "Epoch [2860], val_loss: 7915.5640\n",
      "Epoch [2880], val_loss: 7915.6816\n",
      "Epoch [2900], val_loss: 7913.6440\n",
      "Epoch [2920], val_loss: 7913.1504\n",
      "Epoch [2940], val_loss: 7918.3271\n",
      "Epoch [2960], val_loss: 7912.1343\n",
      "Epoch [2980], val_loss: 7912.7412\n",
      "Epoch [3000], val_loss: 7909.9775\n",
      "Epoch [3020], val_loss: 7909.8340\n",
      "Epoch [3040], val_loss: 7908.6323\n",
      "Epoch [3060], val_loss: 7907.9004\n",
      "Epoch [3080], val_loss: 7909.5430\n",
      "Epoch [3100], val_loss: 7909.2734\n",
      "Epoch [3120], val_loss: 7905.8594\n",
      "Epoch [3140], val_loss: 7906.1836\n",
      "Epoch [3160], val_loss: 7904.4912\n",
      "Epoch [3180], val_loss: 7907.5420\n",
      "Epoch [3200], val_loss: 7902.7070\n",
      "Epoch [3220], val_loss: 7901.8965\n",
      "Epoch [3240], val_loss: 7901.9404\n",
      "Epoch [3260], val_loss: 7899.9033\n",
      "Epoch [3280], val_loss: 7901.3101\n",
      "Epoch [3300], val_loss: 7900.4585\n",
      "Epoch [3320], val_loss: 7897.7568\n",
      "Epoch [3340], val_loss: 7897.0293\n",
      "Epoch [3360], val_loss: 7896.2119\n",
      "Epoch [3380], val_loss: 7901.3057\n",
      "Epoch [3400], val_loss: 7894.7070\n",
      "Epoch [3420], val_loss: 7895.2734\n",
      "Epoch [3440], val_loss: 7892.7910\n",
      "Epoch [3460], val_loss: 7892.6143\n",
      "Epoch [3480], val_loss: 7891.6885\n",
      "Epoch [3500], val_loss: 7890.6685\n",
      "Epoch [3520], val_loss: 7890.0322\n",
      "Epoch [3540], val_loss: 7890.9697\n",
      "Epoch [3560], val_loss: 7891.8750\n",
      "Epoch [3580], val_loss: 7888.1406\n",
      "Epoch [3600], val_loss: 7887.4785\n",
      "Epoch [3620], val_loss: 7887.3174\n",
      "Epoch [3640], val_loss: 7887.0493\n",
      "Epoch [3660], val_loss: 7884.5942\n",
      "Epoch [3680], val_loss: 7883.5410\n",
      "Epoch [3700], val_loss: 7882.6973\n",
      "Epoch [3720], val_loss: 7882.1182\n",
      "Epoch [3740], val_loss: 7881.1807\n",
      "Epoch [3760], val_loss: 7880.0303\n",
      "Epoch [3780], val_loss: 7879.3848\n",
      "Epoch [3800], val_loss: 7878.6328\n",
      "Epoch [3820], val_loss: 7878.1147\n",
      "Epoch [3840], val_loss: 7877.3569\n",
      "Epoch [3860], val_loss: 7876.3145\n",
      "Epoch [3880], val_loss: 7876.3545\n",
      "Epoch [3900], val_loss: 7875.6094\n",
      "Epoch [3920], val_loss: 7875.7939\n",
      "Epoch [3940], val_loss: 7873.4463\n",
      "Epoch [3960], val_loss: 7873.0117\n",
      "Epoch [3980], val_loss: 7872.2031\n",
      "Epoch [4000], val_loss: 7870.7065\n",
      "Epoch [4020], val_loss: 7871.7275\n",
      "Epoch [4040], val_loss: 7869.4404\n",
      "Epoch [4060], val_loss: 7869.5571\n",
      "Epoch [4080], val_loss: 7868.9033\n",
      "Epoch [4100], val_loss: 7868.4473\n",
      "Epoch [4120], val_loss: 7866.0737\n",
      "Epoch [4140], val_loss: 7866.1035\n",
      "Epoch [4160], val_loss: 7864.7930\n",
      "Epoch [4180], val_loss: 7864.8091\n",
      "Epoch [4200], val_loss: 7863.7124\n",
      "Epoch [4220], val_loss: 7863.0967\n",
      "Epoch [4240], val_loss: 7861.7539\n",
      "Epoch [4260], val_loss: 7861.1074\n",
      "Epoch [4280], val_loss: 7859.9429\n",
      "Epoch [4300], val_loss: 7859.6401\n",
      "Epoch [4320], val_loss: 7859.1836\n",
      "Epoch [4340], val_loss: 7857.5801\n",
      "Epoch [4360], val_loss: 7858.0894\n",
      "Epoch [4380], val_loss: 7856.8574\n",
      "Epoch [4400], val_loss: 7856.0762\n",
      "Epoch [4420], val_loss: 7854.6533\n",
      "Epoch [4440], val_loss: 7854.2178\n",
      "Epoch [4460], val_loss: 7853.0669\n",
      "Epoch [4480], val_loss: 7852.8589\n",
      "Epoch [4500], val_loss: 7851.5088\n",
      "Epoch [4520], val_loss: 7851.1812\n",
      "Epoch [4540], val_loss: 7850.4033\n",
      "Epoch [4560], val_loss: 7848.8848\n",
      "Epoch [4580], val_loss: 7851.6709\n",
      "Epoch [4600], val_loss: 7847.1777\n",
      "Epoch [4620], val_loss: 7849.1748\n",
      "Epoch [4640], val_loss: 7845.9009\n",
      "Epoch [4660], val_loss: 7845.0635\n",
      "Epoch [4680], val_loss: 7844.4395\n",
      "Epoch [4700], val_loss: 7843.5361\n",
      "Epoch [4720], val_loss: 7843.0454\n",
      "Epoch [4740], val_loss: 7842.0439\n",
      "Epoch [4760], val_loss: 7842.5361\n",
      "Epoch [4780], val_loss: 7840.6860\n",
      "Epoch [4800], val_loss: 7840.4082\n",
      "Epoch [4820], val_loss: 7839.3154\n",
      "Epoch [4840], val_loss: 7838.0801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4860], val_loss: 7837.7246\n",
      "Epoch [4880], val_loss: 7836.6025\n",
      "Epoch [4900], val_loss: 7835.6035\n",
      "Epoch [4920], val_loss: 7834.7783\n",
      "Epoch [4940], val_loss: 7834.2061\n",
      "Epoch [4960], val_loss: 7833.4629\n",
      "Epoch [4980], val_loss: 7832.3564\n",
      "Epoch [5000], val_loss: 7832.0205\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 0.1\n",
    "history3 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 6852.8896\n",
      "Epoch [40], val_loss: 6980.0693\n",
      "Epoch [60], val_loss: 7061.2515\n",
      "Epoch [80], val_loss: 6840.8047\n",
      "Epoch [100], val_loss: 6932.3428\n",
      "Epoch [120], val_loss: 6832.3735\n",
      "Epoch [140], val_loss: 6815.3418\n",
      "Epoch [160], val_loss: 6802.7344\n",
      "Epoch [180], val_loss: 6791.2139\n",
      "Epoch [200], val_loss: 6779.4014\n",
      "Epoch [220], val_loss: 6777.5488\n",
      "Epoch [240], val_loss: 6774.7070\n",
      "Epoch [260], val_loss: 6767.6699\n",
      "Epoch [280], val_loss: 6844.6885\n",
      "Epoch [300], val_loss: 6752.7490\n",
      "Epoch [320], val_loss: 6966.3740\n",
      "Epoch [340], val_loss: 6750.9136\n",
      "Epoch [360], val_loss: 6721.3760\n",
      "Epoch [380], val_loss: 6798.1543\n",
      "Epoch [400], val_loss: 6702.7871\n",
      "Epoch [420], val_loss: 6750.7607\n",
      "Epoch [440], val_loss: 6688.6582\n",
      "Epoch [460], val_loss: 6695.0518\n",
      "Epoch [480], val_loss: 6735.4551\n",
      "Epoch [500], val_loss: 6665.0752\n",
      "Epoch [520], val_loss: 6681.2461\n",
      "Epoch [540], val_loss: 6672.0049\n",
      "Epoch [560], val_loss: 6670.1738\n",
      "Epoch [580], val_loss: 6657.0996\n",
      "Epoch [600], val_loss: 6672.6309\n",
      "Epoch [620], val_loss: 6646.2041\n",
      "Epoch [640], val_loss: 6766.4653\n",
      "Epoch [660], val_loss: 6705.7231\n",
      "Epoch [680], val_loss: 6673.9404\n",
      "Epoch [700], val_loss: 6599.1924\n",
      "Epoch [720], val_loss: 6585.3955\n",
      "Epoch [740], val_loss: 6617.7393\n",
      "Epoch [760], val_loss: 6618.8335\n",
      "Epoch [780], val_loss: 6592.9717\n",
      "Epoch [800], val_loss: 6630.5664\n",
      "Epoch [820], val_loss: 6558.3770\n",
      "Epoch [840], val_loss: 6575.3081\n",
      "Epoch [860], val_loss: 6746.7695\n",
      "Epoch [880], val_loss: 6611.9082\n",
      "Epoch [900], val_loss: 6650.0361\n",
      "Epoch [920], val_loss: 6587.3008\n",
      "Epoch [940], val_loss: 6510.1582\n",
      "Epoch [960], val_loss: 6508.4814\n",
      "Epoch [980], val_loss: 6559.6543\n",
      "Epoch [1000], val_loss: 6474.5186\n",
      "Epoch [1020], val_loss: 6779.4849\n",
      "Epoch [1040], val_loss: 6468.1445\n",
      "Epoch [1060], val_loss: 6535.9185\n",
      "Epoch [1080], val_loss: 6468.8018\n",
      "Epoch [1100], val_loss: 6454.0996\n",
      "Epoch [1120], val_loss: 6432.3574\n",
      "Epoch [1140], val_loss: 6483.9756\n",
      "Epoch [1160], val_loss: 6525.9404\n",
      "Epoch [1180], val_loss: 6646.3892\n",
      "Epoch [1200], val_loss: 6429.8994\n",
      "Epoch [1220], val_loss: 6417.4375\n",
      "Epoch [1240], val_loss: 6381.6025\n",
      "Epoch [1260], val_loss: 6389.0762\n",
      "Epoch [1280], val_loss: 6384.4238\n",
      "Epoch [1300], val_loss: 6460.0322\n",
      "Epoch [1320], val_loss: 6694.2344\n",
      "Epoch [1340], val_loss: 6348.2510\n",
      "Epoch [1360], val_loss: 6450.3916\n",
      "Epoch [1380], val_loss: 6410.0918\n",
      "Epoch [1400], val_loss: 6350.8501\n",
      "Epoch [1420], val_loss: 6325.2827\n",
      "Epoch [1440], val_loss: 6308.9629\n",
      "Epoch [1460], val_loss: 6313.4854\n",
      "Epoch [1480], val_loss: 6290.6924\n",
      "Epoch [1500], val_loss: 6285.7246\n",
      "Epoch [1520], val_loss: 6532.3496\n",
      "Epoch [1540], val_loss: 6299.9971\n",
      "Epoch [1560], val_loss: 6280.7612\n",
      "Epoch [1580], val_loss: 6282.1260\n",
      "Epoch [1600], val_loss: 6394.4644\n",
      "Epoch [1620], val_loss: 6287.8730\n",
      "Epoch [1640], val_loss: 6236.2139\n",
      "Epoch [1660], val_loss: 6224.8838\n",
      "Epoch [1680], val_loss: 6232.4971\n",
      "Epoch [1700], val_loss: 6210.7393\n",
      "Epoch [1720], val_loss: 6278.9258\n",
      "Epoch [1740], val_loss: 6389.0703\n",
      "Epoch [1760], val_loss: 6319.7969\n",
      "Epoch [1780], val_loss: 6181.3525\n",
      "Epoch [1800], val_loss: 6175.0479\n",
      "Epoch [1820], val_loss: 6586.7866\n",
      "Epoch [1840], val_loss: 6349.9473\n",
      "Epoch [1860], val_loss: 6151.1250\n",
      "Epoch [1880], val_loss: 6151.9238\n",
      "Epoch [1900], val_loss: 6179.7920\n",
      "Epoch [1920], val_loss: 6132.9658\n",
      "Epoch [1940], val_loss: 6161.9995\n",
      "Epoch [1960], val_loss: 6218.0840\n",
      "Epoch [1980], val_loss: 6107.6030\n",
      "Epoch [2000], val_loss: 6142.8516\n",
      "Epoch [2020], val_loss: 6209.8418\n",
      "Epoch [2040], val_loss: 6079.2139\n",
      "Epoch [2060], val_loss: 6089.8770\n",
      "Epoch [2080], val_loss: 6093.5293\n",
      "Epoch [2100], val_loss: 6100.2031\n",
      "Epoch [2120], val_loss: 6059.5635\n",
      "Epoch [2140], val_loss: 6042.2568\n",
      "Epoch [2160], val_loss: 6038.8291\n",
      "Epoch [2180], val_loss: 6138.7979\n",
      "Epoch [2200], val_loss: 6026.1353\n",
      "Epoch [2220], val_loss: 6044.2861\n",
      "Epoch [2240], val_loss: 6009.2583\n",
      "Epoch [2260], val_loss: 6000.5659\n",
      "Epoch [2280], val_loss: 6025.8584\n",
      "Epoch [2300], val_loss: 6016.0596\n",
      "Epoch [2320], val_loss: 5978.8545\n",
      "Epoch [2340], val_loss: 6114.2300\n",
      "Epoch [2360], val_loss: 5958.5303\n",
      "Epoch [2380], val_loss: 6422.2490\n",
      "Epoch [2400], val_loss: 5957.3208\n",
      "Epoch [2420], val_loss: 6198.9521\n",
      "Epoch [2440], val_loss: 6296.4141\n",
      "Epoch [2460], val_loss: 6114.2637\n",
      "Epoch [2480], val_loss: 6210.5210\n",
      "Epoch [2500], val_loss: 5924.3916\n",
      "Epoch [2520], val_loss: 6207.6182\n",
      "Epoch [2540], val_loss: 5901.1416\n",
      "Epoch [2560], val_loss: 5887.0449\n",
      "Epoch [2580], val_loss: 5880.3394\n",
      "Epoch [2600], val_loss: 5892.6084\n",
      "Epoch [2620], val_loss: 5862.9170\n",
      "Epoch [2640], val_loss: 6064.3389\n",
      "Epoch [2660], val_loss: 5969.3926\n",
      "Epoch [2680], val_loss: 5885.1909\n",
      "Epoch [2700], val_loss: 5920.1816\n",
      "Epoch [2720], val_loss: 5825.3193\n",
      "Epoch [2740], val_loss: 5881.8311\n",
      "Epoch [2760], val_loss: 5814.7847\n",
      "Epoch [2780], val_loss: 5827.7998\n",
      "Epoch [2800], val_loss: 5827.1396\n",
      "Epoch [2820], val_loss: 5908.4434\n",
      "Epoch [2840], val_loss: 5851.8750\n",
      "Epoch [2860], val_loss: 5921.9868\n",
      "Epoch [2880], val_loss: 5775.5938\n",
      "Epoch [2900], val_loss: 5764.4619\n",
      "Epoch [2920], val_loss: 5815.2402\n",
      "Epoch [2940], val_loss: 5750.6772\n",
      "Epoch [2960], val_loss: 5771.1211\n",
      "Epoch [2980], val_loss: 5791.8184\n",
      "Epoch [3000], val_loss: 5718.7773\n",
      "Epoch [3020], val_loss: 5721.9893\n",
      "Epoch [3040], val_loss: 5710.4023\n",
      "Epoch [3060], val_loss: 5761.4463\n",
      "Epoch [3080], val_loss: 5739.4170\n",
      "Epoch [3100], val_loss: 5752.1675\n",
      "Epoch [3120], val_loss: 5695.3857\n",
      "Epoch [3140], val_loss: 5771.3682\n",
      "Epoch [3160], val_loss: 5669.3809\n",
      "Epoch [3180], val_loss: 5685.7881\n",
      "Epoch [3200], val_loss: 5650.1953\n",
      "Epoch [3220], val_loss: 5746.9121\n",
      "Epoch [3240], val_loss: 5635.7666\n",
      "Epoch [3260], val_loss: 5665.3320\n",
      "Epoch [3280], val_loss: 5727.2627\n",
      "Epoch [3300], val_loss: 5613.5908\n",
      "Epoch [3320], val_loss: 5662.8809\n",
      "Epoch [3340], val_loss: 5629.3066\n",
      "Epoch [3360], val_loss: 5647.9150\n",
      "Epoch [3380], val_loss: 5582.3765\n",
      "Epoch [3400], val_loss: 5661.8931\n",
      "Epoch [3420], val_loss: 5568.3213\n",
      "Epoch [3440], val_loss: 5720.8369\n",
      "Epoch [3460], val_loss: 5604.6328\n",
      "Epoch [3480], val_loss: 5838.9580\n",
      "Epoch [3500], val_loss: 5541.0342\n",
      "Epoch [3520], val_loss: 5528.8418\n",
      "Epoch [3540], val_loss: 5619.2070\n",
      "Epoch [3560], val_loss: 5822.4346\n",
      "Epoch [3580], val_loss: 5512.2856\n",
      "Epoch [3600], val_loss: 5596.4463\n",
      "Epoch [3620], val_loss: 5516.4873\n",
      "Epoch [3640], val_loss: 5541.1392\n",
      "Epoch [3660], val_loss: 5483.3696\n",
      "Epoch [3680], val_loss: 5537.3086\n",
      "Epoch [3700], val_loss: 5682.7007\n",
      "Epoch [3720], val_loss: 5542.1953\n",
      "Epoch [3740], val_loss: 5490.3242\n",
      "Epoch [3760], val_loss: 5456.5205\n",
      "Epoch [3780], val_loss: 5469.3477\n",
      "Epoch [3800], val_loss: 5459.0576\n",
      "Epoch [3820], val_loss: 5430.2148\n",
      "Epoch [3840], val_loss: 5710.3369\n",
      "Epoch [3860], val_loss: 5463.1025\n",
      "Epoch [3880], val_loss: 5430.7383\n",
      "Epoch [3900], val_loss: 5515.0664\n",
      "Epoch [3920], val_loss: 5413.5352\n",
      "Epoch [3940], val_loss: 5437.5322\n",
      "Epoch [3960], val_loss: 5412.1172\n",
      "Epoch [3980], val_loss: 5448.8516\n",
      "Epoch [4000], val_loss: 5368.6846\n",
      "Epoch [4020], val_loss: 5388.5630\n",
      "Epoch [4040], val_loss: 5376.8115\n",
      "Epoch [4060], val_loss: 5371.6040\n",
      "Epoch [4080], val_loss: 5548.3047\n",
      "Epoch [4100], val_loss: 5525.4849\n",
      "Epoch [4120], val_loss: 5349.8164\n",
      "Epoch [4140], val_loss: 5594.2471\n",
      "Epoch [4160], val_loss: 5320.6641\n",
      "Epoch [4180], val_loss: 5311.5469\n",
      "Epoch [4200], val_loss: 5405.1865\n",
      "Epoch [4220], val_loss: 5299.1475\n",
      "Epoch [4240], val_loss: 5329.6143\n",
      "Epoch [4260], val_loss: 5344.9893\n",
      "Epoch [4280], val_loss: 5323.1777\n",
      "Epoch [4300], val_loss: 5320.0850\n",
      "Epoch [4320], val_loss: 5265.5601\n",
      "Epoch [4340], val_loss: 5312.3022\n",
      "Epoch [4360], val_loss: 5267.9194\n",
      "Epoch [4380], val_loss: 5472.5200\n",
      "Epoch [4400], val_loss: 5319.8594\n",
      "Epoch [4420], val_loss: 5260.1118\n",
      "Epoch [4440], val_loss: 5247.7939\n",
      "Epoch [4460], val_loss: 5301.3657\n",
      "Epoch [4480], val_loss: 5583.5542\n",
      "Epoch [4500], val_loss: 5232.6323\n",
      "Epoch [4520], val_loss: 5204.1484\n",
      "Epoch [4540], val_loss: 5197.0928\n",
      "Epoch [4560], val_loss: 5187.7729\n",
      "Epoch [4580], val_loss: 5574.7886\n",
      "Epoch [4600], val_loss: 5318.7803\n",
      "Epoch [4620], val_loss: 5268.1035\n",
      "Epoch [4640], val_loss: 5171.5361\n",
      "Epoch [4660], val_loss: 5160.1777\n",
      "Epoch [4680], val_loss: 5206.8320\n",
      "Epoch [4700], val_loss: 5385.1587\n",
      "Epoch [4720], val_loss: 5275.6987\n",
      "Epoch [4740], val_loss: 5141.5293\n",
      "Epoch [4760], val_loss: 5128.6465\n",
      "Epoch [4780], val_loss: 5235.6875\n",
      "Epoch [4800], val_loss: 5203.8662\n",
      "Epoch [4820], val_loss: 5232.4194\n",
      "Epoch [4840], val_loss: 5657.6118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4860], val_loss: 5130.0776\n",
      "Epoch [4880], val_loss: 5166.1953\n",
      "Epoch [4900], val_loss: 5373.5400\n",
      "Epoch [4920], val_loss: 5160.9238\n",
      "Epoch [4940], val_loss: 5222.9658\n",
      "Epoch [4960], val_loss: 5182.7246\n",
      "Epoch [4980], val_loss: 5089.1157\n",
      "Epoch [5000], val_loss: 5065.5859\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 1\n",
    "history4 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss_list = [loss['val_loss'] for loss in history4]\n",
    "plt.plot(val_loss_list)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss') \n",
    "plt.title('Loss vs. epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 5066.5000\n",
      "Epoch [40], val_loss: 5065.9492\n",
      "Epoch [60], val_loss: 5067.1602\n",
      "Epoch [80], val_loss: 5064.3213\n",
      "Epoch [100], val_loss: 5064.4492\n",
      "Epoch [120], val_loss: 5061.6836\n",
      "Epoch [140], val_loss: 5066.7256\n",
      "Epoch [160], val_loss: 5061.0664\n",
      "Epoch [180], val_loss: 5065.3203\n",
      "Epoch [200], val_loss: 5062.5835\n",
      "Epoch [220], val_loss: 5058.8721\n",
      "Epoch [240], val_loss: 5060.2314\n",
      "Epoch [260], val_loss: 5061.8623\n",
      "Epoch [280], val_loss: 5058.5674\n",
      "Epoch [300], val_loss: 5057.5537\n",
      "Epoch [320], val_loss: 5056.6396\n",
      "Epoch [340], val_loss: 5057.4795\n",
      "Epoch [360], val_loss: 5060.5093\n",
      "Epoch [380], val_loss: 5054.6621\n",
      "Epoch [400], val_loss: 5054.0723\n",
      "Epoch [420], val_loss: 5057.1660\n",
      "Epoch [440], val_loss: 5054.3291\n",
      "Epoch [460], val_loss: 5053.1240\n",
      "Epoch [480], val_loss: 5053.0908\n",
      "Epoch [500], val_loss: 5054.0601\n",
      "Epoch [520], val_loss: 5057.4844\n",
      "Epoch [540], val_loss: 5051.6602\n",
      "Epoch [560], val_loss: 5050.5361\n",
      "Epoch [580], val_loss: 5050.8901\n",
      "Epoch [600], val_loss: 5050.5283\n",
      "Epoch [620], val_loss: 5048.4580\n",
      "Epoch [640], val_loss: 5058.3003\n",
      "Epoch [660], val_loss: 5053.4238\n",
      "Epoch [680], val_loss: 5048.9551\n",
      "Epoch [700], val_loss: 5046.6650\n",
      "Epoch [720], val_loss: 5047.8721\n",
      "Epoch [740], val_loss: 5047.1621\n",
      "Epoch [760], val_loss: 5045.2402\n",
      "Epoch [780], val_loss: 5053.2559\n",
      "Epoch [800], val_loss: 5044.0815\n",
      "Epoch [820], val_loss: 5044.6465\n",
      "Epoch [840], val_loss: 5043.5664\n",
      "Epoch [860], val_loss: 5044.7695\n",
      "Epoch [880], val_loss: 5041.8828\n",
      "Epoch [900], val_loss: 5041.9131\n",
      "Epoch [920], val_loss: 5046.4604\n",
      "Epoch [940], val_loss: 5042.1592\n",
      "Epoch [960], val_loss: 5041.6611\n",
      "Epoch [980], val_loss: 5041.0156\n",
      "Epoch [1000], val_loss: 5039.5127\n",
      "Epoch [1020], val_loss: 5039.1914\n",
      "Epoch [1040], val_loss: 5041.1201\n",
      "Epoch [1060], val_loss: 5037.8594\n",
      "Epoch [1080], val_loss: 5040.1992\n",
      "Epoch [1100], val_loss: 5041.0127\n",
      "Epoch [1120], val_loss: 5038.9146\n",
      "Epoch [1140], val_loss: 5035.8984\n",
      "Epoch [1160], val_loss: 5036.1318\n",
      "Epoch [1180], val_loss: 5037.4014\n",
      "Epoch [1200], val_loss: 5035.9634\n",
      "Epoch [1220], val_loss: 5035.5298\n",
      "Epoch [1240], val_loss: 5036.8789\n",
      "Epoch [1260], val_loss: 5033.5400\n",
      "Epoch [1280], val_loss: 5035.6377\n",
      "Epoch [1300], val_loss: 5032.5874\n",
      "Epoch [1320], val_loss: 5031.5752\n",
      "Epoch [1340], val_loss: 5039.4922\n",
      "Epoch [1360], val_loss: 5030.5615\n",
      "Epoch [1380], val_loss: 5036.3838\n",
      "Epoch [1400], val_loss: 5031.0049\n",
      "Epoch [1420], val_loss: 5034.1221\n",
      "Epoch [1440], val_loss: 5032.5107\n",
      "Epoch [1460], val_loss: 5029.0205\n",
      "Epoch [1480], val_loss: 5027.6870\n",
      "Epoch [1500], val_loss: 5031.8262\n",
      "Epoch [1520], val_loss: 5027.3535\n",
      "Epoch [1540], val_loss: 5030.9844\n",
      "Epoch [1560], val_loss: 5026.0571\n",
      "Epoch [1580], val_loss: 5026.9365\n",
      "Epoch [1600], val_loss: 5026.0459\n",
      "Epoch [1620], val_loss: 5030.7861\n",
      "Epoch [1640], val_loss: 5026.9966\n",
      "Epoch [1660], val_loss: 5024.6060\n",
      "Epoch [1680], val_loss: 5023.7197\n",
      "Epoch [1700], val_loss: 5023.5469\n",
      "Epoch [1720], val_loss: 5025.0591\n",
      "Epoch [1740], val_loss: 5021.5273\n",
      "Epoch [1760], val_loss: 5023.1230\n",
      "Epoch [1780], val_loss: 5023.7158\n",
      "Epoch [1800], val_loss: 5024.0742\n",
      "Epoch [1820], val_loss: 5024.9688\n",
      "Epoch [1840], val_loss: 5021.1699\n",
      "Epoch [1860], val_loss: 5021.2534\n",
      "Epoch [1880], val_loss: 5020.8906\n",
      "Epoch [1900], val_loss: 5017.9780\n",
      "Epoch [1920], val_loss: 5017.2363\n",
      "Epoch [1940], val_loss: 5017.8828\n",
      "Epoch [1960], val_loss: 5017.3184\n",
      "Epoch [1980], val_loss: 5019.2656\n",
      "Epoch [2000], val_loss: 5018.0156\n",
      "Epoch [2020], val_loss: 5016.4336\n",
      "Epoch [2040], val_loss: 5015.0264\n",
      "Epoch [2060], val_loss: 5018.7715\n",
      "Epoch [2080], val_loss: 5013.6953\n",
      "Epoch [2100], val_loss: 5012.8926\n",
      "Epoch [2120], val_loss: 5012.9048\n",
      "Epoch [2140], val_loss: 5017.3037\n",
      "Epoch [2160], val_loss: 5017.0449\n",
      "Epoch [2180], val_loss: 5014.2461\n",
      "Epoch [2200], val_loss: 5014.3936\n",
      "Epoch [2220], val_loss: 5010.9600\n",
      "Epoch [2240], val_loss: 5011.2959\n",
      "Epoch [2260], val_loss: 5009.5068\n",
      "Epoch [2280], val_loss: 5014.0884\n",
      "Epoch [2300], val_loss: 5012.1660\n",
      "Epoch [2320], val_loss: 5009.7842\n",
      "Epoch [2340], val_loss: 5013.3174\n",
      "Epoch [2360], val_loss: 5011.5439\n",
      "Epoch [2380], val_loss: 5010.0322\n",
      "Epoch [2400], val_loss: 5006.6543\n",
      "Epoch [2420], val_loss: 5008.8066\n",
      "Epoch [2440], val_loss: 5009.6514\n",
      "Epoch [2460], val_loss: 5014.3672\n",
      "Epoch [2480], val_loss: 5006.3115\n",
      "Epoch [2500], val_loss: 5008.8613\n",
      "Epoch [2520], val_loss: 5004.0981\n",
      "Epoch [2540], val_loss: 5003.2100\n",
      "Epoch [2560], val_loss: 5006.2334\n",
      "Epoch [2580], val_loss: 5003.5508\n",
      "Epoch [2600], val_loss: 5001.4307\n",
      "Epoch [2620], val_loss: 5003.6543\n",
      "Epoch [2640], val_loss: 5003.4922\n",
      "Epoch [2660], val_loss: 5001.1328\n",
      "Epoch [2680], val_loss: 4999.4590\n",
      "Epoch [2700], val_loss: 5003.5718\n",
      "Epoch [2720], val_loss: 5002.8213\n",
      "Epoch [2740], val_loss: 5002.8804\n",
      "Epoch [2760], val_loss: 4998.9507\n",
      "Epoch [2780], val_loss: 4998.7310\n",
      "Epoch [2800], val_loss: 5001.1636\n",
      "Epoch [2820], val_loss: 4998.4746\n",
      "Epoch [2840], val_loss: 5004.2817\n",
      "Epoch [2860], val_loss: 4994.9438\n",
      "Epoch [2880], val_loss: 4998.4014\n",
      "Epoch [2900], val_loss: 4994.6318\n",
      "Epoch [2920], val_loss: 4997.6016\n",
      "Epoch [2940], val_loss: 4995.7822\n",
      "Epoch [2960], val_loss: 4992.7998\n",
      "Epoch [2980], val_loss: 4994.4990\n",
      "Epoch [3000], val_loss: 4995.9922\n",
      "Epoch [3020], val_loss: 4992.5151\n",
      "Epoch [3040], val_loss: 4991.3828\n",
      "Epoch [3060], val_loss: 4990.3887\n",
      "Epoch [3080], val_loss: 4991.7568\n",
      "Epoch [3100], val_loss: 4989.5566\n",
      "Epoch [3120], val_loss: 4990.4346\n",
      "Epoch [3140], val_loss: 4997.8955\n",
      "Epoch [3160], val_loss: 4991.5132\n",
      "Epoch [3180], val_loss: 4988.8853\n",
      "Epoch [3200], val_loss: 4986.9521\n",
      "Epoch [3220], val_loss: 4987.4629\n",
      "Epoch [3240], val_loss: 4986.4580\n",
      "Epoch [3260], val_loss: 4991.0234\n",
      "Epoch [3280], val_loss: 4989.6455\n",
      "Epoch [3300], val_loss: 4986.1216\n",
      "Epoch [3320], val_loss: 4984.8428\n",
      "Epoch [3340], val_loss: 4984.2842\n",
      "Epoch [3360], val_loss: 4986.0410\n",
      "Epoch [3380], val_loss: 4988.4658\n",
      "Epoch [3400], val_loss: 4986.9951\n",
      "Epoch [3420], val_loss: 4983.3345\n",
      "Epoch [3440], val_loss: 4982.2979\n",
      "Epoch [3460], val_loss: 4981.4531\n",
      "Epoch [3480], val_loss: 4980.7886\n",
      "Epoch [3500], val_loss: 4983.9355\n",
      "Epoch [3520], val_loss: 4980.4707\n",
      "Epoch [3540], val_loss: 4982.6201\n",
      "Epoch [3560], val_loss: 4979.6748\n",
      "Epoch [3580], val_loss: 4979.5327\n",
      "Epoch [3600], val_loss: 4981.1963\n",
      "Epoch [3620], val_loss: 4978.5962\n",
      "Epoch [3640], val_loss: 4978.1245\n",
      "Epoch [3660], val_loss: 4976.8447\n",
      "Epoch [3680], val_loss: 4979.0718\n",
      "Epoch [3700], val_loss: 4982.9023\n",
      "Epoch [3720], val_loss: 4975.6172\n",
      "Epoch [3740], val_loss: 4979.4854\n",
      "Epoch [3760], val_loss: 4975.9111\n",
      "Epoch [3780], val_loss: 4978.7373\n",
      "Epoch [3800], val_loss: 4975.0010\n",
      "Epoch [3820], val_loss: 4975.4976\n",
      "Epoch [3840], val_loss: 4975.5586\n",
      "Epoch [3860], val_loss: 4972.3511\n",
      "Epoch [3880], val_loss: 4978.1865\n",
      "Epoch [3900], val_loss: 4973.3125\n",
      "Epoch [3920], val_loss: 4973.0693\n",
      "Epoch [3940], val_loss: 4971.1235\n",
      "Epoch [3960], val_loss: 4969.6445\n",
      "Epoch [3980], val_loss: 4972.4019\n",
      "Epoch [4000], val_loss: 4969.6719\n",
      "Epoch [4020], val_loss: 4974.5410\n",
      "Epoch [4040], val_loss: 4967.8262\n",
      "Epoch [4060], val_loss: 4972.1074\n",
      "Epoch [4080], val_loss: 4971.8281\n",
      "Epoch [4100], val_loss: 4966.3921\n",
      "Epoch [4120], val_loss: 4967.1963\n",
      "Epoch [4140], val_loss: 4965.8379\n",
      "Epoch [4160], val_loss: 4967.4756\n",
      "Epoch [4180], val_loss: 4970.8452\n",
      "Epoch [4200], val_loss: 4966.7822\n",
      "Epoch [4220], val_loss: 4968.3774\n",
      "Epoch [4240], val_loss: 4969.4922\n",
      "Epoch [4260], val_loss: 4967.9229\n",
      "Epoch [4280], val_loss: 4962.8394\n",
      "Epoch [4300], val_loss: 4967.3535\n",
      "Epoch [4320], val_loss: 4964.6768\n",
      "Epoch [4340], val_loss: 4963.7437\n",
      "Epoch [4360], val_loss: 4964.2119\n",
      "Epoch [4380], val_loss: 4963.3096\n",
      "Epoch [4400], val_loss: 4959.5283\n",
      "Epoch [4420], val_loss: 4960.8188\n",
      "Epoch [4440], val_loss: 4961.3281\n",
      "Epoch [4460], val_loss: 4962.0859\n",
      "Epoch [4480], val_loss: 4957.8564\n",
      "Epoch [4500], val_loss: 4959.1763\n",
      "Epoch [4520], val_loss: 4960.8628\n",
      "Epoch [4540], val_loss: 4958.9814\n",
      "Epoch [4560], val_loss: 4963.4502\n",
      "Epoch [4580], val_loss: 4964.0283\n",
      "Epoch [4600], val_loss: 4960.8486\n",
      "Epoch [4620], val_loss: 4957.5923\n",
      "Epoch [4640], val_loss: 4959.9795\n",
      "Epoch [4660], val_loss: 4957.0898\n",
      "Epoch [4680], val_loss: 4958.6265\n",
      "Epoch [4700], val_loss: 4954.8711\n",
      "Epoch [4720], val_loss: 4953.3955\n",
      "Epoch [4740], val_loss: 4951.8086\n",
      "Epoch [4760], val_loss: 4955.4541\n",
      "Epoch [4780], val_loss: 4954.0806\n",
      "Epoch [4800], val_loss: 4957.0723\n",
      "Epoch [4820], val_loss: 4950.3799\n",
      "Epoch [4840], val_loss: 4950.6836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4860], val_loss: 4950.7651\n",
      "Epoch [4880], val_loss: 4951.3560\n",
      "Epoch [4900], val_loss: 4952.7676\n",
      "Epoch [4920], val_loss: 4949.3354\n",
      "Epoch [4940], val_loss: 4949.7495\n",
      "Epoch [4960], val_loss: 4947.6973\n",
      "Epoch [4980], val_loss: 4946.9551\n",
      "Epoch [5000], val_loss: 4946.8857\n",
      "Epoch [5020], val_loss: 4950.6484\n",
      "Epoch [5040], val_loss: 4952.0176\n",
      "Epoch [5060], val_loss: 4945.1187\n",
      "Epoch [5080], val_loss: 4949.8174\n",
      "Epoch [5100], val_loss: 4947.5195\n",
      "Epoch [5120], val_loss: 4948.7354\n",
      "Epoch [5140], val_loss: 4945.6167\n",
      "Epoch [5160], val_loss: 4943.6909\n",
      "Epoch [5180], val_loss: 4947.5049\n",
      "Epoch [5200], val_loss: 4943.5527\n",
      "Epoch [5220], val_loss: 4944.8472\n",
      "Epoch [5240], val_loss: 4942.9326\n",
      "Epoch [5260], val_loss: 4941.0562\n",
      "Epoch [5280], val_loss: 4942.6489\n",
      "Epoch [5300], val_loss: 4946.8765\n",
      "Epoch [5320], val_loss: 4940.9629\n",
      "Epoch [5340], val_loss: 4938.9507\n",
      "Epoch [5360], val_loss: 4942.2686\n",
      "Epoch [5380], val_loss: 4938.4727\n",
      "Epoch [5400], val_loss: 4938.6821\n",
      "Epoch [5420], val_loss: 4943.7314\n",
      "Epoch [5440], val_loss: 4940.0986\n",
      "Epoch [5460], val_loss: 4937.1240\n",
      "Epoch [5480], val_loss: 4938.2622\n",
      "Epoch [5500], val_loss: 4940.3828\n",
      "Epoch [5520], val_loss: 4935.3330\n",
      "Epoch [5540], val_loss: 4939.6055\n",
      "Epoch [5560], val_loss: 4936.9014\n",
      "Epoch [5580], val_loss: 4937.3682\n",
      "Epoch [5600], val_loss: 4935.3203\n",
      "Epoch [5620], val_loss: 4934.1377\n",
      "Epoch [5640], val_loss: 4937.5845\n",
      "Epoch [5660], val_loss: 4934.3989\n",
      "Epoch [5680], val_loss: 4932.4932\n",
      "Epoch [5700], val_loss: 4935.6240\n",
      "Epoch [5720], val_loss: 4934.8081\n",
      "Epoch [5740], val_loss: 4930.8887\n",
      "Epoch [5760], val_loss: 4931.0098\n",
      "Epoch [5780], val_loss: 4930.7183\n",
      "Epoch [5800], val_loss: 4931.5020\n",
      "Epoch [5820], val_loss: 4930.3916\n",
      "Epoch [5840], val_loss: 4928.3716\n",
      "Epoch [5860], val_loss: 4928.7949\n",
      "Epoch [5880], val_loss: 4929.3228\n",
      "Epoch [5900], val_loss: 4934.3682\n",
      "Epoch [5920], val_loss: 4929.8994\n",
      "Epoch [5940], val_loss: 4925.5205\n",
      "Epoch [5960], val_loss: 4927.2451\n",
      "Epoch [5980], val_loss: 4927.8008\n",
      "Epoch [6000], val_loss: 4925.7480\n",
      "Epoch [6020], val_loss: 4923.4067\n",
      "Epoch [6040], val_loss: 4928.2324\n",
      "Epoch [6060], val_loss: 4925.6523\n",
      "Epoch [6080], val_loss: 4925.5684\n",
      "Epoch [6100], val_loss: 4924.4922\n",
      "Epoch [6120], val_loss: 4924.9414\n",
      "Epoch [6140], val_loss: 4928.4312\n",
      "Epoch [6160], val_loss: 4927.6250\n",
      "Epoch [6180], val_loss: 4924.1987\n",
      "Epoch [6200], val_loss: 4920.2598\n",
      "Epoch [6220], val_loss: 4924.2920\n",
      "Epoch [6240], val_loss: 4921.1318\n",
      "Epoch [6260], val_loss: 4923.4502\n",
      "Epoch [6280], val_loss: 4919.3921\n",
      "Epoch [6300], val_loss: 4917.9717\n",
      "Epoch [6320], val_loss: 4919.4902\n",
      "Epoch [6340], val_loss: 4916.9873\n",
      "Epoch [6360], val_loss: 4921.1470\n",
      "Epoch [6380], val_loss: 4919.3340\n",
      "Epoch [6400], val_loss: 4919.4980\n",
      "Epoch [6420], val_loss: 4920.7451\n",
      "Epoch [6440], val_loss: 4916.7393\n",
      "Epoch [6460], val_loss: 4919.2764\n",
      "Epoch [6480], val_loss: 4917.6484\n",
      "Epoch [6500], val_loss: 4919.1963\n",
      "Epoch [6520], val_loss: 4920.0518\n",
      "Epoch [6540], val_loss: 4917.7676\n",
      "Epoch [6560], val_loss: 4912.3809\n",
      "Epoch [6580], val_loss: 4921.9160\n",
      "Epoch [6600], val_loss: 4911.8633\n",
      "Epoch [6620], val_loss: 4915.9492\n",
      "Epoch [6640], val_loss: 4911.9082\n",
      "Epoch [6660], val_loss: 4914.0166\n",
      "Epoch [6680], val_loss: 4909.9746\n",
      "Epoch [6700], val_loss: 4911.5254\n",
      "Epoch [6720], val_loss: 4912.1709\n",
      "Epoch [6740], val_loss: 4908.3174\n",
      "Epoch [6760], val_loss: 4911.6436\n",
      "Epoch [6780], val_loss: 4914.2603\n",
      "Epoch [6800], val_loss: 4907.5605\n",
      "Epoch [6820], val_loss: 4913.1025\n",
      "Epoch [6840], val_loss: 4908.6367\n",
      "Epoch [6860], val_loss: 4908.7070\n",
      "Epoch [6880], val_loss: 4912.3354\n",
      "Epoch [6900], val_loss: 4905.3369\n",
      "Epoch [6920], val_loss: 4914.0986\n",
      "Epoch [6940], val_loss: 4910.3364\n",
      "Epoch [6960], val_loss: 4909.4121\n",
      "Epoch [6980], val_loss: 4905.0293\n",
      "Epoch [7000], val_loss: 4908.3652\n",
      "Epoch [7020], val_loss: 4907.1133\n",
      "Epoch [7040], val_loss: 4907.8877\n",
      "Epoch [7060], val_loss: 4901.7632\n",
      "Epoch [7080], val_loss: 4905.9385\n",
      "Epoch [7100], val_loss: 4907.9429\n",
      "Epoch [7120], val_loss: 4901.1729\n",
      "Epoch [7140], val_loss: 4900.3970\n",
      "Epoch [7160], val_loss: 4907.9561\n",
      "Epoch [7180], val_loss: 4907.7812\n",
      "Epoch [7200], val_loss: 4899.3027\n",
      "Epoch [7220], val_loss: 4903.3584\n",
      "Epoch [7240], val_loss: 4897.9131\n",
      "Epoch [7260], val_loss: 4902.5620\n",
      "Epoch [7280], val_loss: 4897.5166\n",
      "Epoch [7300], val_loss: 4903.1802\n",
      "Epoch [7320], val_loss: 4906.5088\n",
      "Epoch [7340], val_loss: 4900.9556\n",
      "Epoch [7360], val_loss: 4898.7905\n",
      "Epoch [7380], val_loss: 4896.0566\n",
      "Epoch [7400], val_loss: 4899.5215\n",
      "Epoch [7420], val_loss: 4894.6528\n",
      "Epoch [7440], val_loss: 4895.2861\n",
      "Epoch [7460], val_loss: 4893.7725\n",
      "Epoch [7480], val_loss: 4896.0117\n",
      "Epoch [7500], val_loss: 4895.2778\n",
      "Epoch [7520], val_loss: 4894.1113\n",
      "Epoch [7540], val_loss: 4896.7554\n",
      "Epoch [7560], val_loss: 4893.5000\n",
      "Epoch [7580], val_loss: 4894.1016\n",
      "Epoch [7600], val_loss: 4898.4980\n",
      "Epoch [7620], val_loss: 4898.1123\n",
      "Epoch [7640], val_loss: 4892.2061\n",
      "Epoch [7660], val_loss: 4891.7661\n",
      "Epoch [7680], val_loss: 4895.7979\n",
      "Epoch [7700], val_loss: 4897.1528\n",
      "Epoch [7720], val_loss: 4891.8115\n",
      "Epoch [7740], val_loss: 4891.3535\n",
      "Epoch [7760], val_loss: 4890.1772\n",
      "Epoch [7780], val_loss: 4892.9443\n",
      "Epoch [7800], val_loss: 4886.2969\n",
      "Epoch [7820], val_loss: 4887.2671\n",
      "Epoch [7840], val_loss: 4887.3174\n",
      "Epoch [7860], val_loss: 4890.9580\n",
      "Epoch [7880], val_loss: 4890.8711\n",
      "Epoch [7900], val_loss: 4891.5801\n",
      "Epoch [7920], val_loss: 4892.3252\n",
      "Epoch [7940], val_loss: 4888.2583\n",
      "Epoch [7960], val_loss: 4891.0195\n",
      "Epoch [7980], val_loss: 4886.4360\n",
      "Epoch [8000], val_loss: 4887.2607\n",
      "Epoch [8020], val_loss: 4887.9668\n",
      "Epoch [8040], val_loss: 4881.8501\n",
      "Epoch [8060], val_loss: 4886.5625\n",
      "Epoch [8080], val_loss: 4884.8564\n",
      "Epoch [8100], val_loss: 4884.0181\n",
      "Epoch [8120], val_loss: 4885.0508\n",
      "Epoch [8140], val_loss: 4884.4033\n",
      "Epoch [8160], val_loss: 4882.4863\n",
      "Epoch [8180], val_loss: 4883.8613\n",
      "Epoch [8200], val_loss: 4885.8784\n",
      "Epoch [8220], val_loss: 4882.5332\n",
      "Epoch [8240], val_loss: 4879.4375\n",
      "Epoch [8260], val_loss: 4882.1938\n",
      "Epoch [8280], val_loss: 4884.6025\n",
      "Epoch [8300], val_loss: 4882.3008\n",
      "Epoch [8320], val_loss: 4878.3496\n",
      "Epoch [8340], val_loss: 4876.7036\n",
      "Epoch [8360], val_loss: 4876.6392\n",
      "Epoch [8380], val_loss: 4875.0586\n",
      "Epoch [8400], val_loss: 4880.4937\n",
      "Epoch [8420], val_loss: 4879.0732\n",
      "Epoch [8440], val_loss: 4880.0928\n",
      "Epoch [8460], val_loss: 4876.4365\n",
      "Epoch [8480], val_loss: 4876.9541\n",
      "Epoch [8500], val_loss: 4877.3638\n",
      "Epoch [8520], val_loss: 4876.4062\n",
      "Epoch [8540], val_loss: 4876.2627\n",
      "Epoch [8560], val_loss: 4877.2246\n",
      "Epoch [8580], val_loss: 4872.1641\n",
      "Epoch [8600], val_loss: 4877.7295\n",
      "Epoch [8620], val_loss: 4873.9473\n",
      "Epoch [8640], val_loss: 4879.3965\n",
      "Epoch [8660], val_loss: 4874.4658\n",
      "Epoch [8680], val_loss: 4875.1074\n",
      "Epoch [8700], val_loss: 4874.8945\n",
      "Epoch [8720], val_loss: 4873.2148\n",
      "Epoch [8740], val_loss: 4871.2690\n",
      "Epoch [8760], val_loss: 4870.1953\n",
      "Epoch [8780], val_loss: 4868.9648\n",
      "Epoch [8800], val_loss: 4873.2002\n",
      "Epoch [8820], val_loss: 4871.7334\n",
      "Epoch [8840], val_loss: 4873.1919\n",
      "Epoch [8860], val_loss: 4869.0801\n",
      "Epoch [8880], val_loss: 4869.4795\n",
      "Epoch [8900], val_loss: 4869.9155\n",
      "Epoch [8920], val_loss: 4868.0020\n",
      "Epoch [8940], val_loss: 4868.6592\n",
      "Epoch [8960], val_loss: 4866.7070\n",
      "Epoch [8980], val_loss: 4868.8076\n",
      "Epoch [9000], val_loss: 4864.5977\n",
      "Epoch [9020], val_loss: 4870.3145\n",
      "Epoch [9040], val_loss: 4870.1221\n",
      "Epoch [9060], val_loss: 4867.6235\n",
      "Epoch [9080], val_loss: 4872.1182\n",
      "Epoch [9100], val_loss: 4863.1846\n",
      "Epoch [9120], val_loss: 4865.2490\n",
      "Epoch [9140], val_loss: 4870.6104\n",
      "Epoch [9160], val_loss: 4865.4902\n",
      "Epoch [9180], val_loss: 4862.4180\n",
      "Epoch [9200], val_loss: 4871.5508\n",
      "Epoch [9220], val_loss: 4865.1152\n",
      "Epoch [9240], val_loss: 4868.1743\n",
      "Epoch [9260], val_loss: 4858.5684\n",
      "Epoch [9280], val_loss: 4858.9053\n",
      "Epoch [9300], val_loss: 4862.3281\n",
      "Epoch [9320], val_loss: 4859.1729\n",
      "Epoch [9340], val_loss: 4861.7031\n",
      "Epoch [9360], val_loss: 4857.8574\n",
      "Epoch [9380], val_loss: 4859.5728\n",
      "Epoch [9400], val_loss: 4854.4443\n",
      "Epoch [9420], val_loss: 4857.5132\n",
      "Epoch [9440], val_loss: 4857.1221\n",
      "Epoch [9460], val_loss: 4857.0957\n",
      "Epoch [9480], val_loss: 4862.0830\n",
      "Epoch [9500], val_loss: 4860.4038\n",
      "Epoch [9520], val_loss: 4859.0977\n",
      "Epoch [9540], val_loss: 4859.2930\n",
      "Epoch [9560], val_loss: 4853.0405\n",
      "Epoch [9580], val_loss: 4854.8374\n",
      "Epoch [9600], val_loss: 4857.2793\n",
      "Epoch [9620], val_loss: 4853.1572\n",
      "Epoch [9640], val_loss: 4857.9712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9660], val_loss: 4854.3364\n",
      "Epoch [9680], val_loss: 4851.5596\n",
      "Epoch [9700], val_loss: 4853.1675\n",
      "Epoch [9720], val_loss: 4850.8779\n",
      "Epoch [9740], val_loss: 4853.7993\n",
      "Epoch [9760], val_loss: 4851.1094\n",
      "Epoch [9780], val_loss: 4852.9697\n",
      "Epoch [9800], val_loss: 4851.7324\n",
      "Epoch [9820], val_loss: 4847.9604\n",
      "Epoch [9840], val_loss: 4848.0537\n",
      "Epoch [9860], val_loss: 4850.4521\n",
      "Epoch [9880], val_loss: 4853.7910\n",
      "Epoch [9900], val_loss: 4849.8750\n",
      "Epoch [9920], val_loss: 4852.1084\n",
      "Epoch [9940], val_loss: 4851.4229\n",
      "Epoch [9960], val_loss: 4849.3740\n",
      "Epoch [9980], val_loss: 4846.0254\n",
      "Epoch [10000], val_loss: 4848.9414\n",
      "Epoch [10020], val_loss: 4850.3276\n",
      "Epoch [10040], val_loss: 4845.3359\n",
      "Epoch [10060], val_loss: 4850.1191\n",
      "Epoch [10080], val_loss: 4851.8486\n",
      "Epoch [10100], val_loss: 4852.0117\n",
      "Epoch [10120], val_loss: 4845.5591\n",
      "Epoch [10140], val_loss: 4844.8633\n",
      "Epoch [10160], val_loss: 4846.3721\n",
      "Epoch [10180], val_loss: 4851.0957\n",
      "Epoch [10200], val_loss: 4843.1416\n",
      "Epoch [10220], val_loss: 4840.6738\n",
      "Epoch [10240], val_loss: 4845.4146\n",
      "Epoch [10260], val_loss: 4844.1777\n",
      "Epoch [10280], val_loss: 4844.0010\n",
      "Epoch [10300], val_loss: 4839.5859\n",
      "Epoch [10320], val_loss: 4839.8398\n",
      "Epoch [10340], val_loss: 4842.7930\n",
      "Epoch [10360], val_loss: 4843.7891\n",
      "Epoch [10380], val_loss: 4840.3906\n",
      "Epoch [10400], val_loss: 4843.4194\n",
      "Epoch [10420], val_loss: 4837.5693\n",
      "Epoch [10440], val_loss: 4843.2803\n",
      "Epoch [10460], val_loss: 4839.7188\n",
      "Epoch [10480], val_loss: 4842.7446\n",
      "Epoch [10500], val_loss: 4840.7910\n",
      "Epoch [10520], val_loss: 4838.7080\n",
      "Epoch [10540], val_loss: 4838.0000\n",
      "Epoch [10560], val_loss: 4842.5566\n",
      "Epoch [10580], val_loss: 4833.7051\n",
      "Epoch [10600], val_loss: 4834.2461\n",
      "Epoch [10620], val_loss: 4833.9805\n",
      "Epoch [10640], val_loss: 4833.1494\n",
      "Epoch [10660], val_loss: 4834.9326\n",
      "Epoch [10680], val_loss: 4840.2686\n",
      "Epoch [10700], val_loss: 4835.2412\n",
      "Epoch [10720], val_loss: 4834.2461\n",
      "Epoch [10740], val_loss: 4833.0107\n",
      "Epoch [10760], val_loss: 4831.7080\n",
      "Epoch [10780], val_loss: 4833.3350\n",
      "Epoch [10800], val_loss: 4830.9863\n",
      "Epoch [10820], val_loss: 4837.5068\n",
      "Epoch [10840], val_loss: 4832.2217\n",
      "Epoch [10860], val_loss: 4835.6758\n",
      "Epoch [10880], val_loss: 4833.5801\n",
      "Epoch [10900], val_loss: 4829.4434\n",
      "Epoch [10920], val_loss: 4832.2949\n",
      "Epoch [10940], val_loss: 4835.0566\n",
      "Epoch [10960], val_loss: 4830.2129\n",
      "Epoch [10980], val_loss: 4833.2725\n",
      "Epoch [11000], val_loss: 4829.5537\n",
      "Epoch [11020], val_loss: 4829.4668\n",
      "Epoch [11040], val_loss: 4827.1099\n",
      "Epoch [11060], val_loss: 4827.6763\n",
      "Epoch [11080], val_loss: 4828.4307\n",
      "Epoch [11100], val_loss: 4827.3877\n",
      "Epoch [11120], val_loss: 4824.9336\n",
      "Epoch [11140], val_loss: 4825.6377\n",
      "Epoch [11160], val_loss: 4829.5293\n",
      "Epoch [11180], val_loss: 4826.3657\n",
      "Epoch [11200], val_loss: 4826.3438\n",
      "Epoch [11220], val_loss: 4824.9473\n",
      "Epoch [11240], val_loss: 4823.9756\n",
      "Epoch [11260], val_loss: 4825.4790\n",
      "Epoch [11280], val_loss: 4822.6543\n",
      "Epoch [11300], val_loss: 4825.1680\n",
      "Epoch [11320], val_loss: 4827.4312\n",
      "Epoch [11340], val_loss: 4829.4888\n",
      "Epoch [11360], val_loss: 4824.5400\n",
      "Epoch [11380], val_loss: 4821.7432\n",
      "Epoch [11400], val_loss: 4824.2168\n",
      "Epoch [11420], val_loss: 4823.4229\n",
      "Epoch [11440], val_loss: 4820.3838\n",
      "Epoch [11460], val_loss: 4823.1553\n",
      "Epoch [11480], val_loss: 4826.3125\n",
      "Epoch [11500], val_loss: 4821.3389\n",
      "Epoch [11520], val_loss: 4822.1973\n",
      "Epoch [11540], val_loss: 4823.0581\n",
      "Epoch [11560], val_loss: 4822.0820\n",
      "Epoch [11580], val_loss: 4821.0684\n",
      "Epoch [11600], val_loss: 4822.0000\n",
      "Epoch [11620], val_loss: 4821.1719\n",
      "Epoch [11640], val_loss: 4819.7275\n",
      "Epoch [11660], val_loss: 4822.1968\n",
      "Epoch [11680], val_loss: 4818.3354\n",
      "Epoch [11700], val_loss: 4819.1855\n",
      "Epoch [11720], val_loss: 4818.9600\n",
      "Epoch [11740], val_loss: 4817.5864\n",
      "Epoch [11760], val_loss: 4815.4316\n",
      "Epoch [11780], val_loss: 4814.8994\n",
      "Epoch [11800], val_loss: 4816.2061\n",
      "Epoch [11820], val_loss: 4813.9644\n",
      "Epoch [11840], val_loss: 4817.8960\n",
      "Epoch [11860], val_loss: 4816.2207\n",
      "Epoch [11880], val_loss: 4818.3530\n",
      "Epoch [11900], val_loss: 4817.2983\n",
      "Epoch [11920], val_loss: 4815.2422\n",
      "Epoch [11940], val_loss: 4815.9287\n",
      "Epoch [11960], val_loss: 4814.5635\n",
      "Epoch [11980], val_loss: 4811.7515\n",
      "Epoch [12000], val_loss: 4811.0850\n",
      "Epoch [12020], val_loss: 4814.4053\n",
      "Epoch [12040], val_loss: 4813.9351\n",
      "Epoch [12060], val_loss: 4812.3252\n",
      "Epoch [12080], val_loss: 4809.9619\n",
      "Epoch [12100], val_loss: 4811.1206\n",
      "Epoch [12120], val_loss: 4810.0850\n",
      "Epoch [12140], val_loss: 4814.0459\n",
      "Epoch [12160], val_loss: 4809.5317\n",
      "Epoch [12180], val_loss: 4808.4590\n",
      "Epoch [12200], val_loss: 4811.8760\n",
      "Epoch [12220], val_loss: 4813.6699\n",
      "Epoch [12240], val_loss: 4810.3169\n",
      "Epoch [12260], val_loss: 4809.7500\n",
      "Epoch [12280], val_loss: 4808.3882\n",
      "Epoch [12300], val_loss: 4806.4375\n",
      "Epoch [12320], val_loss: 4807.0352\n",
      "Epoch [12340], val_loss: 4808.7065\n",
      "Epoch [12360], val_loss: 4808.0078\n",
      "Epoch [12380], val_loss: 4806.6748\n",
      "Epoch [12400], val_loss: 4808.1196\n",
      "Epoch [12420], val_loss: 4806.8076\n",
      "Epoch [12440], val_loss: 4809.0195\n",
      "Epoch [12460], val_loss: 4806.2358\n",
      "Epoch [12480], val_loss: 4805.7432\n",
      "Epoch [12500], val_loss: 4805.5015\n",
      "Epoch [12520], val_loss: 4805.6943\n",
      "Epoch [12540], val_loss: 4805.9434\n",
      "Epoch [12560], val_loss: 4808.0479\n",
      "Epoch [12580], val_loss: 4805.7041\n",
      "Epoch [12600], val_loss: 4804.6724\n",
      "Epoch [12620], val_loss: 4802.6396\n",
      "Epoch [12640], val_loss: 4804.4976\n",
      "Epoch [12660], val_loss: 4803.3838\n",
      "Epoch [12680], val_loss: 4803.5361\n",
      "Epoch [12700], val_loss: 4802.1914\n",
      "Epoch [12720], val_loss: 4801.2783\n",
      "Epoch [12740], val_loss: 4801.5156\n",
      "Epoch [12760], val_loss: 4800.2979\n",
      "Epoch [12780], val_loss: 4800.8154\n",
      "Epoch [12800], val_loss: 4799.5430\n",
      "Epoch [12820], val_loss: 4800.7373\n",
      "Epoch [12840], val_loss: 4800.2100\n",
      "Epoch [12860], val_loss: 4802.9482\n",
      "Epoch [12880], val_loss: 4802.7935\n",
      "Epoch [12900], val_loss: 4798.7554\n",
      "Epoch [12920], val_loss: 4799.5806\n",
      "Epoch [12940], val_loss: 4798.8979\n",
      "Epoch [12960], val_loss: 4797.6260\n",
      "Epoch [12980], val_loss: 4798.4697\n",
      "Epoch [13000], val_loss: 4798.1646\n",
      "Epoch [13020], val_loss: 4795.8359\n",
      "Epoch [13040], val_loss: 4795.9595\n",
      "Epoch [13060], val_loss: 4798.9067\n",
      "Epoch [13080], val_loss: 4796.8506\n",
      "Epoch [13100], val_loss: 4794.0425\n",
      "Epoch [13120], val_loss: 4795.0078\n",
      "Epoch [13140], val_loss: 4796.5229\n",
      "Epoch [13160], val_loss: 4793.2148\n",
      "Epoch [13180], val_loss: 4798.8633\n",
      "Epoch [13200], val_loss: 4797.6367\n",
      "Epoch [13220], val_loss: 4794.0732\n",
      "Epoch [13240], val_loss: 4794.0918\n",
      "Epoch [13260], val_loss: 4793.4604\n",
      "Epoch [13280], val_loss: 4794.0391\n",
      "Epoch [13300], val_loss: 4793.4971\n",
      "Epoch [13320], val_loss: 4792.2695\n",
      "Epoch [13340], val_loss: 4794.0361\n",
      "Epoch [13360], val_loss: 4793.0371\n",
      "Epoch [13380], val_loss: 4792.5776\n",
      "Epoch [13400], val_loss: 4793.2119\n",
      "Epoch [13420], val_loss: 4792.2852\n",
      "Epoch [13440], val_loss: 4791.1948\n",
      "Epoch [13460], val_loss: 4790.2324\n",
      "Epoch [13480], val_loss: 4794.2861\n",
      "Epoch [13500], val_loss: 4789.0552\n",
      "Epoch [13520], val_loss: 4789.8506\n",
      "Epoch [13540], val_loss: 4790.5537\n",
      "Epoch [13560], val_loss: 4787.1567\n",
      "Epoch [13580], val_loss: 4786.9971\n",
      "Epoch [13600], val_loss: 4787.6646\n",
      "Epoch [13620], val_loss: 4786.7046\n",
      "Epoch [13640], val_loss: 4789.6929\n",
      "Epoch [13660], val_loss: 4786.8740\n",
      "Epoch [13680], val_loss: 4788.2734\n",
      "Epoch [13700], val_loss: 4786.5371\n",
      "Epoch [13720], val_loss: 4786.4976\n",
      "Epoch [13740], val_loss: 4787.3716\n",
      "Epoch [13760], val_loss: 4787.1562\n",
      "Epoch [13780], val_loss: 4788.2480\n",
      "Epoch [13800], val_loss: 4785.1729\n",
      "Epoch [13820], val_loss: 4785.0762\n",
      "Epoch [13840], val_loss: 4783.8262\n",
      "Epoch [13860], val_loss: 4784.6035\n",
      "Epoch [13880], val_loss: 4785.1475\n",
      "Epoch [13900], val_loss: 4783.8125\n",
      "Epoch [13920], val_loss: 4783.7197\n",
      "Epoch [13940], val_loss: 4783.5898\n",
      "Epoch [13960], val_loss: 4782.1836\n",
      "Epoch [13980], val_loss: 4783.9873\n",
      "Epoch [14000], val_loss: 4782.5059\n",
      "Epoch [14020], val_loss: 4781.6362\n",
      "Epoch [14040], val_loss: 4788.9126\n",
      "Epoch [14060], val_loss: 4782.2969\n",
      "Epoch [14080], val_loss: 4783.8066\n",
      "Epoch [14100], val_loss: 4782.2324\n",
      "Epoch [14120], val_loss: 4783.1807\n",
      "Epoch [14140], val_loss: 4785.1328\n",
      "Epoch [14160], val_loss: 4781.2461\n",
      "Epoch [14180], val_loss: 4779.7627\n",
      "Epoch [14200], val_loss: 4779.2363\n",
      "Epoch [14220], val_loss: 4781.4097\n",
      "Epoch [14240], val_loss: 4781.2695\n",
      "Epoch [14260], val_loss: 4783.1089\n",
      "Epoch [14280], val_loss: 4779.7256\n",
      "Epoch [14300], val_loss: 4778.0469\n",
      "Epoch [14320], val_loss: 4780.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14340], val_loss: 4779.8481\n",
      "Epoch [14360], val_loss: 4778.3281\n",
      "Epoch [14380], val_loss: 4777.7012\n",
      "Epoch [14400], val_loss: 4775.9712\n",
      "Epoch [14420], val_loss: 4777.9795\n",
      "Epoch [14440], val_loss: 4779.7529\n",
      "Epoch [14460], val_loss: 4779.0850\n",
      "Epoch [14480], val_loss: 4775.9858\n",
      "Epoch [14500], val_loss: 4779.5156\n",
      "Epoch [14520], val_loss: 4774.3999\n",
      "Epoch [14540], val_loss: 4775.8066\n",
      "Epoch [14560], val_loss: 4773.7871\n",
      "Epoch [14580], val_loss: 4773.1162\n",
      "Epoch [14600], val_loss: 4772.3701\n",
      "Epoch [14620], val_loss: 4773.0747\n",
      "Epoch [14640], val_loss: 4773.7178\n",
      "Epoch [14660], val_loss: 4775.0254\n",
      "Epoch [14680], val_loss: 4775.0928\n",
      "Epoch [14700], val_loss: 4778.1538\n",
      "Epoch [14720], val_loss: 4774.4688\n",
      "Epoch [14740], val_loss: 4772.1025\n",
      "Epoch [14760], val_loss: 4773.6680\n",
      "Epoch [14780], val_loss: 4771.7100\n",
      "Epoch [14800], val_loss: 4771.9961\n",
      "Epoch [14820], val_loss: 4772.7969\n",
      "Epoch [14840], val_loss: 4771.0762\n",
      "Epoch [14860], val_loss: 4770.0713\n",
      "Epoch [14880], val_loss: 4769.3599\n",
      "Epoch [14900], val_loss: 4772.3184\n",
      "Epoch [14920], val_loss: 4769.0957\n",
      "Epoch [14940], val_loss: 4770.4629\n",
      "Epoch [14960], val_loss: 4767.7227\n",
      "Epoch [14980], val_loss: 4770.9224\n",
      "Epoch [15000], val_loss: 4767.4751\n",
      "Epoch [15020], val_loss: 4769.2090\n",
      "Epoch [15040], val_loss: 4769.2002\n",
      "Epoch [15060], val_loss: 4767.5273\n",
      "Epoch [15080], val_loss: 4767.9902\n",
      "Epoch [15100], val_loss: 4766.3486\n",
      "Epoch [15120], val_loss: 4765.7178\n",
      "Epoch [15140], val_loss: 4767.0034\n",
      "Epoch [15160], val_loss: 4768.0557\n",
      "Epoch [15180], val_loss: 4768.3086\n",
      "Epoch [15200], val_loss: 4765.2305\n",
      "Epoch [15220], val_loss: 4764.3892\n",
      "Epoch [15240], val_loss: 4766.3555\n",
      "Epoch [15260], val_loss: 4764.8291\n",
      "Epoch [15280], val_loss: 4765.4556\n",
      "Epoch [15300], val_loss: 4765.3809\n",
      "Epoch [15320], val_loss: 4763.2153\n",
      "Epoch [15340], val_loss: 4764.3687\n",
      "Epoch [15360], val_loss: 4763.2695\n",
      "Epoch [15380], val_loss: 4765.0356\n",
      "Epoch [15400], val_loss: 4765.2134\n",
      "Epoch [15420], val_loss: 4766.9668\n",
      "Epoch [15440], val_loss: 4762.3711\n",
      "Epoch [15460], val_loss: 4761.9854\n",
      "Epoch [15480], val_loss: 4764.0210\n",
      "Epoch [15500], val_loss: 4761.0693\n",
      "Epoch [15520], val_loss: 4761.8281\n",
      "Epoch [15540], val_loss: 4760.7837\n",
      "Epoch [15560], val_loss: 4762.5156\n",
      "Epoch [15580], val_loss: 4761.1699\n",
      "Epoch [15600], val_loss: 4764.4238\n",
      "Epoch [15620], val_loss: 4760.7783\n",
      "Epoch [15640], val_loss: 4759.4463\n",
      "Epoch [15660], val_loss: 4759.4868\n",
      "Epoch [15680], val_loss: 4760.1694\n",
      "Epoch [15700], val_loss: 4761.4263\n",
      "Epoch [15720], val_loss: 4758.5864\n",
      "Epoch [15740], val_loss: 4759.8896\n",
      "Epoch [15760], val_loss: 4758.7480\n",
      "Epoch [15780], val_loss: 4759.8862\n",
      "Epoch [15800], val_loss: 4759.0566\n",
      "Epoch [15820], val_loss: 4758.3477\n",
      "Epoch [15840], val_loss: 4757.1240\n",
      "Epoch [15860], val_loss: 4757.4209\n",
      "Epoch [15880], val_loss: 4757.7168\n",
      "Epoch [15900], val_loss: 4759.5840\n",
      "Epoch [15920], val_loss: 4756.0483\n",
      "Epoch [15940], val_loss: 4755.5518\n",
      "Epoch [15960], val_loss: 4755.7275\n",
      "Epoch [15980], val_loss: 4755.8911\n",
      "Epoch [16000], val_loss: 4757.1870\n",
      "Epoch [16020], val_loss: 4755.1284\n",
      "Epoch [16040], val_loss: 4755.7378\n",
      "Epoch [16060], val_loss: 4754.1035\n",
      "Epoch [16080], val_loss: 4756.4014\n",
      "Epoch [16100], val_loss: 4754.4331\n",
      "Epoch [16120], val_loss: 4755.4141\n",
      "Epoch [16140], val_loss: 4754.1318\n",
      "Epoch [16160], val_loss: 4753.4434\n",
      "Epoch [16180], val_loss: 4752.4941\n",
      "Epoch [16200], val_loss: 4753.8594\n",
      "Epoch [16220], val_loss: 4753.0664\n",
      "Epoch [16240], val_loss: 4752.6948\n",
      "Epoch [16260], val_loss: 4752.5112\n",
      "Epoch [16280], val_loss: 4751.4541\n",
      "Epoch [16300], val_loss: 4751.0098\n",
      "Epoch [16320], val_loss: 4750.5923\n",
      "Epoch [16340], val_loss: 4751.3457\n",
      "Epoch [16360], val_loss: 4751.9297\n",
      "Epoch [16380], val_loss: 4753.4087\n",
      "Epoch [16400], val_loss: 4750.8276\n",
      "Epoch [16420], val_loss: 4750.6592\n",
      "Epoch [16440], val_loss: 4751.3853\n",
      "Epoch [16460], val_loss: 4750.9678\n",
      "Epoch [16480], val_loss: 4749.7349\n",
      "Epoch [16500], val_loss: 4750.8389\n",
      "Epoch [16520], val_loss: 4749.3193\n",
      "Epoch [16540], val_loss: 4750.6255\n",
      "Epoch [16560], val_loss: 4749.4067\n",
      "Epoch [16580], val_loss: 4748.7598\n",
      "Epoch [16600], val_loss: 4748.9746\n",
      "Epoch [16620], val_loss: 4752.3052\n",
      "Epoch [16640], val_loss: 4751.2520\n",
      "Epoch [16660], val_loss: 4747.1313\n",
      "Epoch [16680], val_loss: 4748.3252\n",
      "Epoch [16700], val_loss: 4747.1484\n",
      "Epoch [16720], val_loss: 4747.7993\n",
      "Epoch [16740], val_loss: 4746.8384\n",
      "Epoch [16760], val_loss: 4747.2012\n",
      "Epoch [16780], val_loss: 4745.1475\n",
      "Epoch [16800], val_loss: 4745.2471\n",
      "Epoch [16820], val_loss: 4744.4111\n",
      "Epoch [16840], val_loss: 4745.9053\n",
      "Epoch [16860], val_loss: 4744.3340\n",
      "Epoch [16880], val_loss: 4744.5342\n",
      "Epoch [16900], val_loss: 4747.8369\n",
      "Epoch [16920], val_loss: 4744.9697\n",
      "Epoch [16940], val_loss: 4747.5762\n",
      "Epoch [16960], val_loss: 4744.7607\n",
      "Epoch [16980], val_loss: 4743.0093\n",
      "Epoch [17000], val_loss: 4743.6880\n",
      "Epoch [17020], val_loss: 4743.8779\n",
      "Epoch [17040], val_loss: 4745.5059\n",
      "Epoch [17060], val_loss: 4742.8467\n",
      "Epoch [17080], val_loss: 4741.7788\n",
      "Epoch [17100], val_loss: 4742.9258\n",
      "Epoch [17120], val_loss: 4741.6689\n",
      "Epoch [17140], val_loss: 4741.2285\n",
      "Epoch [17160], val_loss: 4743.7480\n",
      "Epoch [17180], val_loss: 4741.6475\n",
      "Epoch [17200], val_loss: 4741.3750\n",
      "Epoch [17220], val_loss: 4740.5225\n",
      "Epoch [17240], val_loss: 4742.6133\n",
      "Epoch [17260], val_loss: 4740.3740\n",
      "Epoch [17280], val_loss: 4740.0908\n",
      "Epoch [17300], val_loss: 4740.4404\n",
      "Epoch [17320], val_loss: 4740.4463\n",
      "Epoch [17340], val_loss: 4739.4507\n",
      "Epoch [17360], val_loss: 4739.7441\n",
      "Epoch [17380], val_loss: 4741.0049\n",
      "Epoch [17400], val_loss: 4739.7988\n",
      "Epoch [17420], val_loss: 4741.7314\n",
      "Epoch [17440], val_loss: 4737.0679\n",
      "Epoch [17460], val_loss: 4740.9375\n",
      "Epoch [17480], val_loss: 4739.4395\n",
      "Epoch [17500], val_loss: 4737.4336\n",
      "Epoch [17520], val_loss: 4737.0254\n",
      "Epoch [17540], val_loss: 4741.6572\n",
      "Epoch [17560], val_loss: 4737.1772\n",
      "Epoch [17580], val_loss: 4736.7754\n",
      "Epoch [17600], val_loss: 4736.3896\n",
      "Epoch [17620], val_loss: 4735.8301\n",
      "Epoch [17640], val_loss: 4736.8677\n",
      "Epoch [17660], val_loss: 4737.5537\n",
      "Epoch [17680], val_loss: 4735.1719\n",
      "Epoch [17700], val_loss: 4733.7100\n",
      "Epoch [17720], val_loss: 4733.7773\n",
      "Epoch [17740], val_loss: 4734.9980\n",
      "Epoch [17760], val_loss: 4733.3945\n",
      "Epoch [17780], val_loss: 4734.1318\n",
      "Epoch [17800], val_loss: 4733.9116\n",
      "Epoch [17820], val_loss: 4735.5874\n",
      "Epoch [17840], val_loss: 4735.7607\n",
      "Epoch [17860], val_loss: 4733.9399\n",
      "Epoch [17880], val_loss: 4732.3633\n",
      "Epoch [17900], val_loss: 4732.7134\n",
      "Epoch [17920], val_loss: 4731.9248\n",
      "Epoch [17940], val_loss: 4732.4482\n",
      "Epoch [17960], val_loss: 4731.6680\n",
      "Epoch [17980], val_loss: 4733.6899\n",
      "Epoch [18000], val_loss: 4732.7598\n",
      "Epoch [18020], val_loss: 4730.4043\n",
      "Epoch [18040], val_loss: 4731.2227\n",
      "Epoch [18060], val_loss: 4733.9590\n",
      "Epoch [18080], val_loss: 4730.7856\n",
      "Epoch [18100], val_loss: 4729.7178\n",
      "Epoch [18120], val_loss: 4732.1230\n",
      "Epoch [18140], val_loss: 4729.7031\n",
      "Epoch [18160], val_loss: 4733.2422\n",
      "Epoch [18180], val_loss: 4732.1470\n",
      "Epoch [18200], val_loss: 4730.3418\n",
      "Epoch [18220], val_loss: 4729.6406\n",
      "Epoch [18240], val_loss: 4727.8262\n",
      "Epoch [18260], val_loss: 4729.6582\n",
      "Epoch [18280], val_loss: 4729.8125\n",
      "Epoch [18300], val_loss: 4730.0127\n",
      "Epoch [18320], val_loss: 4729.9199\n",
      "Epoch [18340], val_loss: 4726.9180\n",
      "Epoch [18360], val_loss: 4727.0850\n",
      "Epoch [18380], val_loss: 4726.4170\n",
      "Epoch [18400], val_loss: 4726.7480\n",
      "Epoch [18420], val_loss: 4729.1079\n",
      "Epoch [18440], val_loss: 4729.2031\n",
      "Epoch [18460], val_loss: 4726.6670\n",
      "Epoch [18480], val_loss: 4726.1899\n",
      "Epoch [18500], val_loss: 4724.6045\n",
      "Epoch [18520], val_loss: 4727.7520\n",
      "Epoch [18540], val_loss: 4725.2939\n",
      "Epoch [18560], val_loss: 4723.8154\n",
      "Epoch [18580], val_loss: 4724.2319\n",
      "Epoch [18600], val_loss: 4724.2314\n",
      "Epoch [18620], val_loss: 4724.0117\n",
      "Epoch [18640], val_loss: 4723.6416\n",
      "Epoch [18660], val_loss: 4723.5239\n",
      "Epoch [18680], val_loss: 4726.8535\n",
      "Epoch [18700], val_loss: 4723.4565\n",
      "Epoch [18720], val_loss: 4722.9150\n",
      "Epoch [18740], val_loss: 4722.6348\n",
      "Epoch [18760], val_loss: 4722.2642\n",
      "Epoch [18780], val_loss: 4724.3584\n",
      "Epoch [18800], val_loss: 4722.9062\n",
      "Epoch [18820], val_loss: 4722.1074\n",
      "Epoch [18840], val_loss: 4721.6367\n",
      "Epoch [18860], val_loss: 4721.0610\n",
      "Epoch [18880], val_loss: 4723.9424\n",
      "Epoch [18900], val_loss: 4721.1938\n",
      "Epoch [18920], val_loss: 4724.3779\n",
      "Epoch [18940], val_loss: 4719.8398\n",
      "Epoch [18960], val_loss: 4722.2578\n",
      "Epoch [18980], val_loss: 4719.1035\n",
      "Epoch [19000], val_loss: 4719.9136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19020], val_loss: 4721.1807\n",
      "Epoch [19040], val_loss: 4723.3984\n",
      "Epoch [19060], val_loss: 4719.1055\n",
      "Epoch [19080], val_loss: 4721.6416\n",
      "Epoch [19100], val_loss: 4723.8467\n",
      "Epoch [19120], val_loss: 4717.8560\n",
      "Epoch [19140], val_loss: 4719.2207\n",
      "Epoch [19160], val_loss: 4719.3145\n",
      "Epoch [19180], val_loss: 4718.2109\n",
      "Epoch [19200], val_loss: 4717.5967\n",
      "Epoch [19220], val_loss: 4717.2383\n",
      "Epoch [19240], val_loss: 4717.0059\n",
      "Epoch [19260], val_loss: 4719.9419\n",
      "Epoch [19280], val_loss: 4716.2251\n",
      "Epoch [19300], val_loss: 4716.3711\n",
      "Epoch [19320], val_loss: 4715.8457\n",
      "Epoch [19340], val_loss: 4717.0474\n",
      "Epoch [19360], val_loss: 4717.5396\n",
      "Epoch [19380], val_loss: 4715.8052\n",
      "Epoch [19400], val_loss: 4715.7637\n",
      "Epoch [19420], val_loss: 4716.5435\n",
      "Epoch [19440], val_loss: 4714.0791\n",
      "Epoch [19460], val_loss: 4716.6660\n",
      "Epoch [19480], val_loss: 4714.7471\n",
      "Epoch [19500], val_loss: 4715.2344\n",
      "Epoch [19520], val_loss: 4714.3403\n",
      "Epoch [19540], val_loss: 4714.5781\n",
      "Epoch [19560], val_loss: 4713.8760\n",
      "Epoch [19580], val_loss: 4714.7051\n",
      "Epoch [19600], val_loss: 4716.3896\n",
      "Epoch [19620], val_loss: 4713.0938\n",
      "Epoch [19640], val_loss: 4716.2109\n",
      "Epoch [19660], val_loss: 4712.3892\n",
      "Epoch [19680], val_loss: 4712.4692\n",
      "Epoch [19700], val_loss: 4711.9307\n",
      "Epoch [19720], val_loss: 4714.2661\n",
      "Epoch [19740], val_loss: 4711.5039\n",
      "Epoch [19760], val_loss: 4712.6572\n",
      "Epoch [19780], val_loss: 4710.9312\n",
      "Epoch [19800], val_loss: 4713.7783\n",
      "Epoch [19820], val_loss: 4710.9385\n",
      "Epoch [19840], val_loss: 4711.9253\n",
      "Epoch [19860], val_loss: 4710.9287\n",
      "Epoch [19880], val_loss: 4712.6240\n",
      "Epoch [19900], val_loss: 4709.9648\n",
      "Epoch [19920], val_loss: 4711.4404\n",
      "Epoch [19940], val_loss: 4709.7617\n",
      "Epoch [19960], val_loss: 4710.6006\n",
      "Epoch [19980], val_loss: 4709.0923\n",
      "Epoch [20000], val_loss: 4710.8662\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "lr = 0.1\n",
    "history5 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the final validation loss of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4710.8662109375"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = history5[-1]['val_loss']\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log the final validation loss to Jovian and commit the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Metrics logged.\n"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics(val_loss=val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ankitgadge250/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions using the trained model\n",
    "\n",
    "**Q: Complete the following function definition to make predictions on a single input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                # fill this\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"Input:\", input)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([57.0000,  0.0000, 22.1100,  1.0000,  0.0000])\n",
      "Target: tensor([12874.5889])\n",
      "Prediction: tensor([12846.8379])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[0]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([24.0000,  1.0000, 39.4460,  0.0000,  0.0000])\n",
      "Target: tensor([2126.0188])\n",
      "Prediction: tensor([2750.3557])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[10]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([52.0000,  0.0000, 34.9030,  2.0000,  0.0000])\n",
      "Target: tensor([11970.7930])\n",
      "Prediction: tensor([11795.0361])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[23]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you happy with your model's predictions? Try to improve them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Step 6: Try another dataset & blog about it\n",
    "\n",
    "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to replicate this notebook for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patterns in machine learning from problem-specific details.You can use one of these starer notebooks (just change the dataset):\n",
    "\n",
    "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n",
    "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n",
    "\n",
    "Here are some sources to find good datasets:\n",
    "\n",
    "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n",
    "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n",
    "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n",
    "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n",
    "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "- https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n",
    "\n",
    "- Interesting title & subtitle\n",
    "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n",
    "- Downloading & exploring the data\n",
    "- Preparing the data for training\n",
    "- Creating a model using PyTorch\n",
    "- Training the model to fit the data\n",
    "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n",
    "- Making predictions using the model\n",
    "\n",
    "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n",
    "\n",
    "Don't forget to share your work on the forum: https://jovian.ai/forum/t/linear-regression-and-logistic-regression-notebooks-and-blog-posts/14039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ankitgadge250/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)\n",
    "jovian.commit(project=project_name, environment=None) # try again, kaggle fails sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ankitgadge250/02-insurance-linear-regression\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\n",
      "[jovian] Committed successfully! https://jovian.ai/ankitgadge250/02-insurance-linear-regression\n",
      "[jovian] Submitting assignment..\n",
      "[jovian] Verify your submission at https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans/assignment/assignment-2-train-your-first-model\n"
     ]
    }
   ],
   "source": [
    "jovian.submit(assignment=\"zerotogans-a2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
